{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1201361e-326f-4a3c-bfec-287bb6dd09da",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-tutorials/multiagent_competitive_ddpg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315b3bb-d5c2-4b6d-be8e-ed2680ba47b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d791be-3c65-4557-b40f-33032a13e03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lilly/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "\n",
    "from vmas import render_interactively\n",
    "from vmas.simulator.core import Agent, Landmark, Line, Sphere, World\n",
    "from vmas.simulator.scenario import BaseScenario\n",
    "from vmas.simulator.utils import Color, ScenarioUtils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensordict import TensorDictBase\n",
    "\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch import multiprocessing\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyMemmapStorage, RandomSampler, ReplayBuffer\n",
    "\n",
    "from torchrl.envs import (\n",
    "    check_env_specs,\n",
    "    ExplorationType,\n",
    "    PettingZooEnv,\n",
    "    RewardSum,\n",
    "    set_exploration_type,\n",
    "    TransformedEnv,\n",
    "    VmasEnv,\n",
    ")\n",
    "\n",
    "from torchrl.modules import (\n",
    "    AdditiveGaussianModule,\n",
    "    MultiAgentMLP,\n",
    "    ProbabilisticActor,\n",
    "    TanhDelta,\n",
    ")\n",
    "\n",
    "from torchrl.objectives import DDPGLoss, SoftUpdate, ValueEstimators\n",
    "\n",
    "from torchrl.record import CSVLogger, PixelRenderTransform, VideoRecorder\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c4391-73e4-4406-8693-a890d5f46725",
   "metadata": {},
   "source": [
    "# GENERAL\n",
    "Setting up of constants such as the seed, the device and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dac0bc9-9d98-43c3-bf57-e3630e5530cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Devices\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 1000 # Number of team frames collected per sampling iteration\n",
    "n_iters = 100  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "\n",
    "# Replay buffer\n",
    "memory_size = 1_000_000  # The replay buffer of each group can store this many frames\n",
    "\n",
    "# Training\n",
    "n_optimiser_steps = 100  # Number of optimization steps per training iteration\n",
    "train_batch_size = 128  # Number of frames trained in each optimiser step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "\n",
    "# DDPG\n",
    "gamma = 0.99  # Discount factor\n",
    "polyak_tau = 0.005  # Tau for the soft-update of the target network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fdfff9-e738-4f42-84e5-01ac57693e84",
   "metadata": {},
   "source": [
    "# CREATING THE ENVIRONMENT\n",
    " Creates a standard simple_tag environment with the specified number of chasers and evaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045d86a0-a417-496c-8a56-5b4bd1892cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Color.RED if adversary else "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363c76aa-8cb4-4e33-9457-d29d6733dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100  # Environment steps before done\n",
    "\n",
    "n_chasers = 2\n",
    "n_evaders = 10\n",
    "n_obstacles = 2\n",
    "\n",
    "num_vmas_envs = (\n",
    "    frames_per_batch // max_steps\n",
    ")  # Number of vectorized environments. frames_per_batch collection will be divided among these environments\n",
    "base_env = VmasEnv(\n",
    "    scenario=PredationScenario(),\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=True,\n",
    "    max_steps=max_steps,\n",
    "    device=device,\n",
    "    seed=seed,\n",
    "    # Scenario specific\n",
    "    num_good_agents=n_evaders,\n",
    "    num_adversaries=n_chasers,\n",
    "    num_landmarks=n_obstacles,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "297b74c2-c030-480b-aa3b-0d6f97a19846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_map: {'adversary': ['adversary_0', 'adversary_1'], 'agent': ['agent_0', 'agent_1', 'agent_2', 'agent_3', 'agent_4', 'agent_5', 'agent_6', 'agent_7', 'agent_8', 'agent_9']}\n"
     ]
    }
   ],
   "source": [
    "print(f\"group_map: {base_env.group_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2e2a88-8d6f-49e6-b4c2-86b5cb64377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: Composite(\n",
      "    adversary: Composite(\n",
      "        action: BoundedContinuous(\n",
      "            shape=torch.Size([10, 2, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 2, 2]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 2, 2]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 2])),\n",
      "    agent: Composite(\n",
      "        action: BoundedContinuous(\n",
      "            shape=torch.Size([10, 10, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 10, 2]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 10, 2]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 10])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([10]))\n",
      "reward_spec: Composite(\n",
      "    adversary: Composite(\n",
      "        reward: UnboundedContinuous(\n",
      "            shape=torch.Size([10, 2, 1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 2])),\n",
      "    agent: Composite(\n",
      "        reward: UnboundedContinuous(\n",
      "            shape=torch.Size([10, 10, 1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 10, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 10, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 10])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([10]))\n",
      "done_spec: Composite(\n",
      "    done: Categorical(\n",
      "        shape=torch.Size([10, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: Categorical(\n",
      "        shape=torch.Size([10, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([10]))\n",
      "observation_spec: Composite(\n",
      "    adversary: Composite(\n",
      "        observation: UnboundedContinuous(\n",
      "            shape=torch.Size([10, 2, 50]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 2, 50]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 2, 50]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 2])),\n",
      "    agent: Composite(\n",
      "        observation: UnboundedContinuous(\n",
      "            shape=torch.Size([10, 10, 48]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 10, 48]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 10, 48]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 10])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([10]))\n"
     ]
    }
   ],
   "source": [
    "print(\"action_spec:\", base_env.full_action_spec)\n",
    "print(\"reward_spec:\", base_env.full_reward_spec)\n",
    "print(\"done_spec:\", base_env.full_done_spec)\n",
    "print(\"observation_spec:\", base_env.observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22835b86-67a4-45bd-808b-3c0822fcf856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_keys: [('adversary', 'action'), ('agent', 'action')]\n",
      "reward_keys: [('adversary', 'reward'), ('agent', 'reward')]\n",
      "done_keys: ['done', 'terminated']\n"
     ]
    }
   ],
   "source": [
    "print(\"action_keys:\", base_env.action_keys)\n",
    "print(\"reward_keys:\", base_env.reward_keys)\n",
    "print(\"done_keys:\", base_env.done_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c6c92-9f55-4698-b134-bcdaa6b73679",
   "metadata": {},
   "source": [
    "## ENVIRONMENT TRANSFORMATION\n",
    "Transforms the environment so that it computes the sum of the reward and resets it when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d054c360-bcb0-4adb-8be5-ef4841802722",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    RewardSum(\n",
    "        in_keys=base_env.reward_keys,\n",
    "        reset_keys=[\"_reset\"] * len(base_env.group_map.keys()),\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc091e9-e4a8-43cc-8cb6-a8a114d85b37",
   "metadata": {},
   "source": [
    "## ENVIRONMENT SPECS TEST\n",
    "Makes sure that the environment fulfills the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53ebef1-cbcf-46c8-b440-194424962ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 15:24:16,102 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3cbb7-4514-4ab9-9b35-a60910e7beb7",
   "metadata": {},
   "source": [
    "# TEST ROLLOUT\n",
    "Tests the environment specification by performing a small dummy rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42434206-38d4-4420-8abb-8ea036bff532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of 5 steps: TensorDict(\n",
      "    fields={\n",
      "        adversary: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([10, 5, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 5, 2, 50]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 5, 2]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        agent: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([10, 5, 10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 5, 10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 5, 10, 48]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 5, 10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                adversary: TensorDict(\n",
      "                    fields={\n",
      "                        episode_reward: Tensor(shape=torch.Size([10, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        observation: Tensor(shape=torch.Size([10, 5, 2, 50]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([10, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([10, 5, 2]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                agent: TensorDict(\n",
      "                    fields={\n",
      "                        episode_reward: Tensor(shape=torch.Size([10, 5, 10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        observation: Tensor(shape=torch.Size([10, 5, 10, 48]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([10, 5, 10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([10, 5, 10]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([10, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10, 5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "Shape of the rollout TensorDict: torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "n_rollout_steps = 5\n",
    "rollout = env.rollout(n_rollout_steps)\n",
    "print(f\"rollout of {n_rollout_steps} steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f127c2-169c-45bc-a6d9-69cc05444599",
   "metadata": {},
   "source": [
    "# POLICY\n",
    "Creates the policy module for each agent group and sets up a policy for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ff4d15-3c18-4f9f-b4af-8cbf4a36eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_modules = {}\n",
    "for group, agents in env.group_map.items():\n",
    "    share_parameters_policy = True  # Can change this based on the group\n",
    "\n",
    "    policy_net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[group, \"observation\"].shape[\n",
    "            -1\n",
    "        ],  # n_obs_per_agent\n",
    "        n_agent_outputs=env.full_action_spec[group, \"action\"].shape[\n",
    "            -1\n",
    "        ],  # n_actions_per_agents\n",
    "        n_agents=len(agents),  # Number of agents in the group\n",
    "        centralised=False,  # the policies are decentralised (i.e., each agent will act from its local observation)\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    )\n",
    "\n",
    "    # Wrap the neural network in a :class:`~tensordict.nn.TensorDictModule`.\n",
    "    # This is simply a module that will read the ``in_keys`` from a tensordict, feed them to the\n",
    "    # neural networks, and write the\n",
    "    # outputs in-place at the ``out_keys``.\n",
    "\n",
    "    policy_module = TensorDictModule(\n",
    "        policy_net,\n",
    "        in_keys=[(group, \"observation\")],\n",
    "        out_keys=[(group, \"param\")],\n",
    "    )  # We just name the input and output that the network will read and write to the input tensordict\n",
    "    policy_modules[group] = policy_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "194c77f5-c50d-4027-a2de-bce3c558720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    policy = ProbabilisticActor(\n",
    "        module=policy_modules[group],\n",
    "        spec=env.full_action_spec[group, \"action\"],\n",
    "        in_keys=[(group, \"param\")],\n",
    "        out_keys=[(group, \"action\")],\n",
    "        distribution_class=TanhDelta,\n",
    "        distribution_kwargs={\n",
    "            \"low\": env.full_action_spec_unbatched[group, \"action\"].space.low,\n",
    "            \"high\": env.full_action_spec_unbatched[group, \"action\"].space.high,\n",
    "        },\n",
    "        return_log_prob=False,\n",
    "    )\n",
    "    policies[group] = policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e322ded-94f4-4f0a-bcab-572d0c930721",
   "metadata": {},
   "source": [
    "# EXPLORATION POLICIES\n",
    "Creates the exploration policies because DDMPG is deterministic and we still want to include some exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b83b1681-7a93-4861-93cb-7e31d2c9043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_policies = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    exploration_policy = TensorDictSequential(\n",
    "        policies[group],\n",
    "        AdditiveGaussianModule(\n",
    "            spec=policies[group].spec,\n",
    "            annealing_num_steps=total_frames\n",
    "            // 2,  # Number of frames after which sigma is sigma_end\n",
    "            action_key=(group, \"action\"),\n",
    "            sigma_init=0.9,  # Initial value of the sigma\n",
    "            sigma_end=0.1,  # Final value of the sigma\n",
    "        ),\n",
    "    )\n",
    "    exploration_policies[group] = exploration_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363c165-8c1f-4973-84ac-c7ad3cd352db",
   "metadata": {},
   "source": [
    "# CRITICS\n",
    "Creates the critics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b218217-1a9d-4b19-8a3c-738ec2d5bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "critics = {}\n",
    "for group, agents in env.group_map.items():\n",
    "    share_parameters_critic = True  # Can change for each group\n",
    "    MADDPG = True  # IDDPG if False, can change for each group\n",
    "\n",
    "    # This module applies the lambda function: reading the action and observation entries for the group\n",
    "    # and concatenating them in a new ``(group, \"obs_action\")`` entry\n",
    "    cat_module = TensorDictModule(\n",
    "        lambda obs, action: torch.cat([obs, action], dim=-1),\n",
    "        in_keys=[(group, \"observation\"), (group, \"action\")],\n",
    "        out_keys=[(group, \"obs_action\")],\n",
    "    )\n",
    "\n",
    "    critic_module = TensorDictModule(\n",
    "        module=MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[group, \"observation\"].shape[-1]\n",
    "            + env.full_action_spec[group, \"action\"].shape[-1],\n",
    "            n_agent_outputs=1,  # 1 value per agent\n",
    "            n_agents=len(agents),\n",
    "            centralised=MADDPG,\n",
    "            share_params=share_parameters_critic,\n",
    "            device=device,\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=torch.nn.Tanh,\n",
    "        ),\n",
    "        in_keys=[(group, \"obs_action\")],  # Read ``(group, \"obs_action\")``\n",
    "        out_keys=[\n",
    "            (group, \"state_action_value\")\n",
    "        ],  # Write ``(group, \"state_action_value\")``\n",
    "    )\n",
    "\n",
    "    critics[group] = TensorDictSequential(\n",
    "        cat_module, critic_module\n",
    "    )  # Run them in sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e8f3ede-27fa-48a6-af12-97d7f9c4c1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running value and policy for group 'adversary': TensorDict(\n",
      "    fields={\n",
      "        adversary: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([10, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                obs_action: Tensor(shape=torch.Size([10, 2, 52]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 2, 50]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                param: Tensor(shape=torch.Size([10, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                state_action_value: Tensor(shape=torch.Size([10, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 2]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        agent: TensorDict(\n",
      "            fields={\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 10, 48]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "Running value and policy for group 'agent': TensorDict(\n",
      "    fields={\n",
      "        adversary: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([10, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                obs_action: Tensor(shape=torch.Size([10, 2, 52]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 2, 50]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                param: Tensor(shape=torch.Size([10, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                state_action_value: Tensor(shape=torch.Size([10, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 2]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        agent: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([10, 10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                obs_action: Tensor(shape=torch.Size([10, 10, 50]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 10, 48]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                param: Tensor(shape=torch.Size([10, 10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                state_action_value: Tensor(shape=torch.Size([10, 10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "reset_td = env.reset()\n",
    "for group, _agents in env.group_map.items():\n",
    "    print(\n",
    "        f\"Running value and policy for group '{group}':\",\n",
    "        critics[group](policies[group](reset_td)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff98a3-2ae5-4ff7-91fc-b3322d4f4fb2",
   "metadata": {},
   "source": [
    "# COLLECTOR\n",
    "Creates the data collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a9cbad-fb79-4d5d-8c41-b44461f9097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put exploration policies from each group in a sequence\n",
    "agents_exploration_policy = TensorDictSequential(*exploration_policies.values())\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    agents_exploration_policy,\n",
    "    device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e03bd1-fb48-4d4e-a2c2-37839a8bcde2",
   "metadata": {},
   "source": [
    "# REPLAY BUFFERS\n",
    "Creates the replay buffers for the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df9d98d-ca35-406c-bddb-e55d2addeaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffers = {}\n",
    "scratch_dirs = []\n",
    "for group, _agents in env.group_map.items():\n",
    "    scratch_dir = tempfile.TemporaryDirectory().name\n",
    "    scratch_dirs.append(scratch_dir)\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        storage=LazyMemmapStorage(\n",
    "            memory_size,\n",
    "            scratch_dir=scratch_dir,\n",
    "        ),  # We will store up to memory_size multi-agent transitions\n",
    "        sampler=RandomSampler(),\n",
    "        batch_size=train_batch_size,  # We will sample batches of this size\n",
    "    )\n",
    "    if device.type != \"cpu\":\n",
    "        replay_buffer.append_transform(lambda x: x.to(device))\n",
    "    replay_buffers[group] = replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e947ba7-0d36-4646-bbd0-61e5c470dfc3",
   "metadata": {},
   "source": [
    "# OPTIMIZERS\n",
    "sets up the loss functions and optimizers for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ea6a8b4-fede-4e70-ab2f-476000b9ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    loss_module = DDPGLoss(\n",
    "        actor_network=policies[group],  # Use the non-explorative policies\n",
    "        value_network=critics[group],\n",
    "        delay_value=True,  # Whether to use a target network for the value\n",
    "        loss_function=\"l2\",\n",
    "    )\n",
    "    loss_module.set_keys(\n",
    "        state_action_value=(group, \"state_action_value\"),\n",
    "        reward=(group, \"reward\"),\n",
    "        done=(group, \"done\"),\n",
    "        terminated=(group, \"terminated\"),\n",
    "    )\n",
    "    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
    "\n",
    "    losses[group] = loss_module\n",
    "\n",
    "target_updaters = {\n",
    "    group: SoftUpdate(loss, tau=polyak_tau) for group, loss in losses.items()\n",
    "}\n",
    "\n",
    "optimisers = {\n",
    "    group: {\n",
    "        \"loss_actor\": torch.optim.Adam(\n",
    "            loss.actor_network_params.flatten_keys().values(), lr=lr\n",
    "        ),\n",
    "        \"loss_value\": torch.optim.Adam(\n",
    "            loss.value_network_params.flatten_keys().values(), lr=lr\n",
    "        ),\n",
    "    }\n",
    "    for group, loss in losses.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2ca9c-a29b-4535-bcc5-4158daa25669",
   "metadata": {},
   "source": [
    "# HELPER METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9ea3024-601e-4289-8fd0-748659c567c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch: TensorDictBase) -> TensorDictBase:\n",
    "    \"\"\"\n",
    "    If the `(group, \"terminated\")` and `(group, \"done\")` keys are not present, create them by expanding\n",
    "    `\"terminated\"` and `\"done\"`.\n",
    "    This is needed to present them with the same shape as the reward to the loss.\n",
    "    \"\"\"\n",
    "    for group in env.group_map.keys():\n",
    "        keys = list(batch.keys(True, True))\n",
    "        group_shape = batch.get_item_shape(group)\n",
    "        nested_done_key = (\"next\", group, \"done\")\n",
    "        nested_terminated_key = (\"next\", group, \"terminated\")\n",
    "        if nested_done_key not in keys:\n",
    "            batch.set(\n",
    "                nested_done_key,\n",
    "                batch.get((\"next\", \"done\")).unsqueeze(-1).expand((*group_shape, 1)),\n",
    "            )\n",
    "        if nested_terminated_key not in keys:\n",
    "            batch.set(\n",
    "                nested_terminated_key,\n",
    "                batch.get((\"next\", \"terminated\"))\n",
    "                .unsqueeze(-1)\n",
    "                .expand((*group_shape, 1)),\n",
    "            )\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30960af0-d33a-4c23-a643-581df40f0734",
   "metadata": {},
   "source": [
    "# TRAINING\n",
    "Trains the two groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d144c102-54da-489a-bed4-b9d092295825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_reward_mean_adversary = 271.0, episode_reward_mean_agent = -27.100000381469727:  43%|▍| 43/100 [04:29<05:58,  6.3"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed while executing module '0'. Scroll up for more info.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[31mRuntimeError\u001b[39m: TensorDictModule failed with operation\n    MultiAgentMLP(\n        MLP(\n          (0): Linear(in_features=48, out_features=256, bias=True)\n          (1): Tanh()\n          (2): Linear(in_features=256, out_features=256, bias=True)\n          (3): Tanh()\n          (4): Linear(in_features=256, out_features=2, bias=True)\n        ),\n        n_agents=10,\n        share_params=True,\n        centralized=False,\n        agent_dim=-2)\n    in_keys=[('agent', 'observation')]\n    out_keys=[('agent', 'param')].",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/sequence.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, tensordict, tensordict_out, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m                 \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    568\u001b[39m                     module_num_or_key = self._get_module_num_or_key(module)\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m                     raise RuntimeError(\n\u001b[32m    570\u001b[39m                         f\"Failed while executing module '{module_num_or_key}'. Scroll up for more info.\"\n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/sequence.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, module, tensordict, **kwargs)\u001b[39m\n\u001b[32m    516\u001b[39m             key \u001b[38;5;28;01min\u001b[39;00m tensordict.keys(include_nested=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;28;01min\u001b[39;00m module.in_keys\n\u001b[32m    517\u001b[39m         ):\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m             tensordict = module(tensordict, **kwargs)\n\u001b[32m    519\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self.partial_tolerant \u001b[38;5;28;01mand\u001b[39;00m isinstance(tensordict, LazyStackedTensorDict):\n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._call_impl(*args, **kwargs)\n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1748\u001b[39m                 \u001b[38;5;28;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;28;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m                 \u001b[38;5;28;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;28;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1751\u001b[39m \n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/common.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \n\u001b[32m    326\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m _self \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m func(_self, tensordict, *args, **kwargs)\n\u001b[32m    328\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(tensordict, *args, **kwargs)\n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_self, tensordict, *args, **kwargs)\u001b[39m\n\u001b[32m    373\u001b[39m                 result = func(_self, tensordict, *args, **kwargs)\n\u001b[32m    374\u001b[39m             \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m                 _skip_existing.set_mode(self.prev)\n\u001b[32m    376\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/common.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[39m\n\u001b[32m   1185\u001b[39m             in_keys = indent(f\"in_keys={self.in_keys}\", \u001b[32m4\u001b[39m * \u001b[33m\" \"\u001b[39m)\n\u001b[32m   1186\u001b[39m             out_keys = indent(f\"out_keys={self.out_keys}\", \u001b[32m4\u001b[39m * \u001b[33m\" \"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1187\u001b[39m             raise err from RuntimeError(\n\u001b[32m   1188\u001b[39m                 f\"TensorDictModule failed with operation\\n{module}\\n{in_keys}\\n{out_keys}.\"\n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/common.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[39m\n\u001b[32m   1185\u001b[39m             in_keys = indent(f\"in_keys={self.in_keys}\", \u001b[32m4\u001b[39m * \u001b[33m\" \"\u001b[39m)\n\u001b[32m   1186\u001b[39m             out_keys = indent(f\"out_keys={self.out_keys}\", \u001b[32m4\u001b[39m * \u001b[33m\" \"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1187\u001b[39m             raise err from RuntimeError(\n\u001b[32m   1188\u001b[39m                 f\"TensorDictModule failed with operation\\n{module}\\n{in_keys}\\n{out_keys}.\"\n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/common.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[39m\n\u001b[32m   1185\u001b[39m             in_keys = indent(f\"in_keys={self.in_keys}\", \u001b[32m4\u001b[39m * \u001b[33m\" \"\u001b[39m)\n\u001b[32m   1186\u001b[39m             out_keys = indent(f\"out_keys={self.out_keys}\", \u001b[32m4\u001b[39m * \u001b[33m\" \"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1187\u001b[39m             raise err from RuntimeError(\n\u001b[32m   1188\u001b[39m                 f\"TensorDictModule failed with operation\\n{module}\\n{in_keys}\\n{out_keys}.\"\n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/common.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, tensors, **kwargs)\u001b[39m\n\u001b[32m   1107\u001b[39m         self, tensors: Sequence[Tensor], **kwargs: Any\n\u001b[32m   1108\u001b[39m     ) -> Tensor | Sequence[Tensor]:\n\u001b[32m-> \u001b[39m\u001b[32m1109\u001b[39m         out = self.module(*tensors, **kwargs)\n\u001b[32m   1110\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._call_impl(*args, **kwargs)\n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1748\u001b[39m                 \u001b[38;5;28;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;28;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m                 \u001b[38;5;28;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;28;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1751\u001b[39m \n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torchrl/modules/models/multiagent.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *inputs)\u001b[39m\n\u001b[32m    153\u001b[39m         \u001b[38;5;66;03m# If parameters are shared, agents use the same network\u001b[39;00m\n\u001b[32m    154\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m self.params.to_module(self._empty_net):\n\u001b[32m    156\u001b[39m                 output = self._empty_net(inputs)\n",
      "\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m __bool__(self) -> bool:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(\u001b[33m\"Converting a tensordict to boolean value is not permitted\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Converting a tensordict to boolean value is not permitted",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_optimiser_steps):\n\u001b[32m     30\u001b[39m     subdata = replay_buffers[group].sample()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     loss_vals = \u001b[43mlosses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m loss_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mloss_actor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mloss_value\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     34\u001b[39m         loss = loss_vals[loss_name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1845\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1845\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1847\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1849\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1850\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1793\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1790\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1791\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1795\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1796\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1797\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1798\u001b[39m     ):\n\u001b[32m   1799\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torchrl/objectives/common.py:55\u001b[39m, in \u001b[36m_forward_wrapper.<locals>.new_forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     53\u001b[39m rm.\u001b[34m__enter__\u001b[39m()\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     57\u001b[39m     em.\u001b[34m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/common.py:327\u001b[39m, in \u001b[36mdispatch.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _self \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(tensordict, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torchrl/objectives/ddpg.py:300\u001b[39m, in \u001b[36mDDPGLoss.forward\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Computes the DDPG losses given a tensordict sampled from the replay buffer.\u001b[39;00m\n\u001b[32m    287\u001b[39m \n\u001b[32m    288\u001b[39m \u001b[33;03mThis function will also write a \"td_error\" key that can be used by prioritized replay buffers to assign\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    297\u001b[39m \n\u001b[32m    298\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    299\u001b[39m loss_value, metadata = \u001b[38;5;28mself\u001b[39m.loss_value(tensordict)\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m loss_actor, metadata_actor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m metadata.update(metadata_actor)\n\u001b[32m    302\u001b[39m td_out = TensorDict(\n\u001b[32m    303\u001b[39m     source={\u001b[33m\"\u001b[39m\u001b[33mloss_actor\u001b[39m\u001b[33m\"\u001b[39m: loss_actor, \u001b[33m\"\u001b[39m\u001b[33mloss_value\u001b[39m\u001b[33m\"\u001b[39m: loss_value, **metadata},\n\u001b[32m    304\u001b[39m     batch_size=[],\n\u001b[32m    305\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torchrl/objectives/ddpg.py:324\u001b[39m, in \u001b[36mDDPGLoss.loss_actor\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m    320\u001b[39m td_copy = tensordict.select(\n\u001b[32m    321\u001b[39m     *\u001b[38;5;28mself\u001b[39m.actor_in_keys, *\u001b[38;5;28mself\u001b[39m.value_exclusive_keys, strict=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    322\u001b[39m ).detach()\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.actor_network_params.to_module(\u001b[38;5;28mself\u001b[39m.actor_network):\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     td_copy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactor_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtd_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cached_detached_value_params.to_module(\u001b[38;5;28mself\u001b[39m.value_network):\n\u001b[32m    326\u001b[39m     td_copy = \u001b[38;5;28mself\u001b[39m.value_network(td_copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/common.py:327\u001b[39m, in \u001b[36mdispatch.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _self \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(tensordict, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/utils.py:373\u001b[39m, in \u001b[36m_set_skip_existing_None.__call__.<locals>.wrapper\u001b[39m\u001b[34m(_self, tensordict, *args, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;28mself\u001b[39m.prev = _skip_existing.get_mode()\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    375\u001b[39m     _skip_existing.set_mode(\u001b[38;5;28mself\u001b[39m.prev)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/probabilistic.py:1318\u001b[39m, in \u001b[36mProbabilisticTensorDictSequential.forward\u001b[39m\u001b[34m(self, tensordict, tensordict_out, **kwargs)\u001b[39m\n\u001b[32m   1314\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1315\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed while executing module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_num_or_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Scroll up for more info.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1316\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1318\u001b[39m     tensordict_exec = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dist_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_exec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1319\u001b[39m     tensordict_exec = \u001b[38;5;28mself\u001b[39m._last_module(\n\u001b[32m   1320\u001b[39m         tensordict_exec, _requires_sample=\u001b[38;5;28mself\u001b[39m._requires_sample\n\u001b[32m   1321\u001b[39m     )\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensordict_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/probabilistic.py:1072\u001b[39m, in \u001b[36mProbabilisticTensorDictSequential.get_dist_params\u001b[39m\u001b[34m(self, tensordict, tensordict_out, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCould not find a default interaction in the modules.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_interaction_type(\u001b[38;5;28mtype\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/common.py:327\u001b[39m, in \u001b[36mdispatch.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _self \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(tensordict, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/utils.py:373\u001b[39m, in \u001b[36m_set_skip_existing_None.__call__.<locals>.wrapper\u001b[39m\u001b[34m(_self, tensordict, *args, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;28mself\u001b[39m.prev = _skip_existing.get_mode()\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    375\u001b[39m     _skip_existing.set_mode(\u001b[38;5;28mself\u001b[39m.prev)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/tensordict/nn/sequence.py:569\u001b[39m, in \u001b[36mTensorDictSequential.forward\u001b[39m\u001b[34m(self, tensordict, tensordict_out, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    568\u001b[39m             module_num_or_key = \u001b[38;5;28mself\u001b[39m._get_module_num_or_key(module)\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    570\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed while executing module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_num_or_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Scroll up for more info.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    571\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    573\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    574\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTensorDictSequential does not support keyword arguments other than \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtensordict_out\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or in_keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.in_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs.keys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    575\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed while executing module '0'. Scroll up for more info."
     ]
    }
   ],
   "source": [
    "pbar = tqdm(\n",
    "    total=n_iters,\n",
    "    desc=\", \".join(\n",
    "        [f\"episode_reward_mean_{group} = 0\" for group in env.group_map.keys()]\n",
    "    ),\n",
    ")\n",
    "episode_reward_mean_map = {group: [] for group in env.group_map.keys()}\n",
    "train_group_map = copy.deepcopy(env.group_map)\n",
    "\n",
    "# Training/collection iterations\n",
    "for iteration, batch in enumerate(collector):\n",
    "    current_frames = batch.numel()\n",
    "    batch = process_batch(batch)  # Util to expand done keys if needed\n",
    "    # Loop over groups\n",
    "    for group in train_group_map.keys():\n",
    "        group_batch = batch.exclude(\n",
    "            *[\n",
    "                key\n",
    "                for _group in env.group_map.keys()\n",
    "                if _group != group\n",
    "                for key in [_group, (\"next\", _group)]\n",
    "            ]\n",
    "        )  # Exclude data from other groups\n",
    "        group_batch = group_batch.reshape(\n",
    "            -1\n",
    "        )  # This just affects the leading dimensions in batch_size of the tensordict\n",
    "        replay_buffers[group].extend(group_batch)\n",
    "\n",
    "        for _ in range(n_optimiser_steps):\n",
    "            subdata = replay_buffers[group].sample()\n",
    "            loss_vals = losses[group](subdata)\n",
    "\n",
    "            for loss_name in [\"loss_actor\", \"loss_value\"]:\n",
    "                loss = loss_vals[loss_name]\n",
    "                optimiser = optimisers[group][loss_name]\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                # Optional\n",
    "                params = optimiser.param_groups[0][\"params\"]\n",
    "                torch.nn.utils.clip_grad_norm_(params, max_grad_norm)\n",
    "\n",
    "                optimiser.step()\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "            # Soft-update the target network\n",
    "            target_updaters[group].step()\n",
    "\n",
    "        # Exploration sigma anneal update\n",
    "        exploration_policies[group][-1].step(current_frames)\n",
    "\n",
    "    # Stop training a certain group when a condition is met (e.g., number of training iterations)\n",
    "    if iteration == iteration_when_stop_training_evaders:\n",
    "        del train_group_map[\"agent\"]\n",
    "\n",
    "    # Logging\n",
    "    for group in env.group_map.keys():\n",
    "        episode_reward_mean = (\n",
    "            batch.get((\"next\", group, \"episode_reward\"))[\n",
    "                batch.get((\"next\", group, \"done\"))\n",
    "            ]\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        episode_reward_mean_map[group].append(episode_reward_mean)\n",
    "\n",
    "    pbar.set_description(\n",
    "        \", \".join(\n",
    "            [\n",
    "                f\"episode_reward_mean_{group} = {episode_reward_mean_map[group][-1]}\"\n",
    "                for group in env.group_map.keys()\n",
    "            ]\n",
    "        ),\n",
    "        refresh=False,\n",
    "    )\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126e664-ea46-432e-bda5-174383890490",
   "metadata": {},
   "source": [
    "# VISUALISATION\n",
    "Visualizes the mean reward per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6f83b-557a-413d-92a6-c37f275ae9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "for i, group in enumerate(env.group_map.keys()):\n",
    "    axs[i].plot(episode_reward_mean_map[group], label=f\"Episode reward mean {group}\")\n",
    "    axs[i].set_ylabel(\"Reward\")\n",
    "    axs[i].axvline(\n",
    "        x=iteration_when_stop_training_evaders,\n",
    "        label=\"Agent (evader) stop training\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    axs[i].legend()\n",
    "axs[-1].set_xlabel(\"Training iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794ef34-856c-4a80-9e08-d74ce35a60ed",
   "metadata": {},
   "source": [
    "## Video\n",
    "Creates and saves a video replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e62e86-5743-4857-97a7-6c7127963964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace tmpdir with any desired path where the video should be saved\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    print(f\"tmp: {tmpdir}\")\n",
    "    tmpdir = \"/home/lilly/dev/marl-predator-prey/marl-predator-prey/videos\"\n",
    "    video_logger = CSVLogger(\"vmas_logs\", tmpdir, video_format=\"mp4\")\n",
    "    print(\"Creating rendering env\")\n",
    "    env_with_render = TransformedEnv(env.base_env, env.transform.clone())\n",
    "    env_with_render = env_with_render.append_transform(\n",
    "        PixelRenderTransform(\n",
    "            out_keys=[\"pixels\"],\n",
    "            # the np.ndarray has a negative stride and needs to be copied before being cast to a tensor\n",
    "            preproc=lambda x: x.copy(),\n",
    "            as_non_tensor=True,\n",
    "            # asking for array rather than on-screen rendering\n",
    "            mode=\"rgb_array\",\n",
    "        )\n",
    "    )\n",
    "    env_with_render = env_with_render.append_transform(\n",
    "        VideoRecorder(logger=video_logger, tag=\"vmas_rendered\")\n",
    "    )\n",
    "    with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
    "        print(\"Rendering rollout...\")\n",
    "        env_with_render.rollout(100, policy=agents_exploration_policy)\n",
    "    print(\"Saving the video...\")\n",
    "    env_with_render.transform.dump()\n",
    "    print(\"Saved! Saved directory tree:\")\n",
    "    video_logger.print_log_dir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18b25d-eb45-4990-95dc-b98e828feed5",
   "metadata": {},
   "source": [
    "# CLEANUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4600cf-19b1-4edc-a591-e612e2b37344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove scratch dir\n",
    "try:\n",
    "    import shutil\n",
    "\n",
    "    for scratch_dir in scratch_dirs:\n",
    "        # Use shutil.rmtree() to delete the directory and all its contents\n",
    "        shutil.rmtree(scratch_dir)\n",
    "        print(f\"Directory '{scratch_dir}' deleted successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory '{scratch_dir}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting directory: {e}\")\n",
    "# sphinx_gallery_end_ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d346936-fa20-490d-8010-2b3d54372a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
