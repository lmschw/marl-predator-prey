{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1201361e-326f-4a3c-bfec-287bb6dd09da",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-tutorials/multiagent_competitive_ddpg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315b3bb-d5c2-4b6d-be8e-ed2680ba47b5",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d791be-3c65-4557-b40f-33032a13e03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lilly/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "\n",
    "from vmas import render_interactively\n",
    "from vmas.simulator.core import Agent, Landmark, Line, Sphere, World\n",
    "from vmas.simulator.scenario import BaseScenario\n",
    "from vmas.simulator.utils import Color, ScenarioUtils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensordict import TensorDictBase\n",
    "\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch import multiprocessing\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyMemmapStorage, RandomSampler, ReplayBuffer\n",
    "\n",
    "from torchrl.envs import (\n",
    "    check_env_specs,\n",
    "    ExplorationType,\n",
    "    PettingZooEnv,\n",
    "    RewardSum,\n",
    "    set_exploration_type,\n",
    "    TransformedEnv,\n",
    "    VmasEnv,\n",
    ")\n",
    "\n",
    "from torchrl.modules import (\n",
    "    AdditiveGaussianModule,\n",
    "    MultiAgentMLP,\n",
    "    ProbabilisticActor,\n",
    "    TanhDelta,\n",
    ")\n",
    "\n",
    "from torchrl.objectives import DDPGLoss, SoftUpdate, ValueEstimators\n",
    "\n",
    "from torchrl.record import CSVLogger, PixelRenderTransform, VideoRecorder\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c4391-73e4-4406-8693-a890d5f46725",
   "metadata": {},
   "source": [
    "# GENERAL\n",
    "Setting up of constants such as the seed, the device and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dac0bc9-9d98-43c3-bf57-e3630e5530cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Devices\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 1000 # Number of team frames collected per sampling iteration\n",
    "n_iters = 100  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# We will stop training the evaders after this many iterations,\n",
    "# should be 0 <= iteration_when_stop_training_evaders <= n_iters\n",
    "iteration_when_stop_training_evaders = n_iters // 2\n",
    "\n",
    "# Replay buffer\n",
    "memory_size = 1_000_000  # The replay buffer of each group can store this many frames\n",
    "\n",
    "# Training\n",
    "n_optimiser_steps = 100  # Number of optimization steps per training iteration\n",
    "train_batch_size = 128  # Number of frames trained in each optimiser step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "\n",
    "# DDPG\n",
    "gamma = 0.99  # Discount factor\n",
    "polyak_tau = 0.005  # Tau for the soft-update of the target network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fdfff9-e738-4f42-84e5-01ac57693e84",
   "metadata": {},
   "source": [
    "# CREATING THE ENVIRONMENT\n",
    " Creates a standard simple_tag environment with the specified number of chasers and evaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045d86a0-a417-496c-8a56-5b4bd1892cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredationScenario(BaseScenario):\n",
    "    def make_world(self, batch_dim: int, device: torch.device, **kwargs):\n",
    "        num_good_agents = kwargs.pop(\"num_good_agents\", 1)\n",
    "        num_adversaries = kwargs.pop(\"num_adversaries\", 3)\n",
    "        num_landmarks = kwargs.pop(\"num_landmarks\", 2)\n",
    "        self.shape_agent_rew = kwargs.pop(\"shape_agent_rew\", False)\n",
    "        self.shape_adversary_rew = kwargs.pop(\"shape_adversary_rew\", False)\n",
    "        self.agents_share_rew = kwargs.pop(\"agents_share_rew\", False)\n",
    "        self.adversaries_share_rew = kwargs.pop(\"adversaries_share_rew\", True)\n",
    "        self.observe_same_team = kwargs.pop(\"observe_same_team\", True)\n",
    "        self.observe_pos = kwargs.pop(\"observe_pos\", True)\n",
    "        self.observe_vel = kwargs.pop(\"observe_vel\", True)\n",
    "        self.bound = kwargs.pop(\"bound\", 1.0)\n",
    "        self.respawn_at_catch = kwargs.pop(\"respawn_at_catch\", False)\n",
    "        ScenarioUtils.check_kwargs_consumed(kwargs)\n",
    "\n",
    "        self.visualize_semidims = False\n",
    "\n",
    "        world = World(\n",
    "            batch_dim=batch_dim,\n",
    "            device=device,\n",
    "            x_semidim=self.bound,\n",
    "            y_semidim=self.bound,\n",
    "            substeps=10,\n",
    "            collision_force=500,\n",
    "        )\n",
    "        # set any world properties first\n",
    "        num_agents = num_adversaries + num_good_agents\n",
    "        self.adversary_radius = 0.075\n",
    "\n",
    "        # Add agents\n",
    "        for i in range(num_agents):\n",
    "            adversary = True if i < num_adversaries else False\n",
    "            name = f\"adversary_{i}\" if adversary else f\"agent_{i - num_adversaries}\"\n",
    "            agent = Agent(\n",
    "                name=name,\n",
    "                collide=True,\n",
    "                shape=Sphere(radius=self.adversary_radius if adversary else 0.05),\n",
    "                u_multiplier=3.0 if adversary else 4.0,\n",
    "                max_speed=1.0 if adversary else 1.3,\n",
    "                color=Color.RED if adversary else Color.GREEN,\n",
    "                adversary=adversary,\n",
    "            )\n",
    "            world.add_agent(agent)\n",
    "        # Add landmarks\n",
    "        for i in range(num_landmarks):\n",
    "            landmark = Landmark(\n",
    "                name=f\"landmark {i}\",\n",
    "                collide=True,\n",
    "                shape=Sphere(radius=0.2),\n",
    "                color=Color.BLACK,\n",
    "            )\n",
    "            world.add_landmark(landmark)\n",
    "\n",
    "        return world\n",
    "\n",
    "    def reset_world_at(self, env_index: int = None):\n",
    "        for agent in self.world.agents:\n",
    "            agent.set_pos(\n",
    "                torch.zeros(\n",
    "                    (\n",
    "                        (1, self.world.dim_p)\n",
    "                        if env_index is not None\n",
    "                        else (self.world.batch_dim, self.world.dim_p)\n",
    "                    ),\n",
    "                    device=self.world.device,\n",
    "                    dtype=torch.float32,\n",
    "                ).uniform_(\n",
    "                    -self.bound,\n",
    "                    self.bound,\n",
    "                ),\n",
    "                batch_index=env_index,\n",
    "            )\n",
    "\n",
    "        for landmark in self.world.landmarks:\n",
    "            landmark.set_pos(\n",
    "                torch.zeros(\n",
    "                    (\n",
    "                        (1, self.world.dim_p)\n",
    "                        if env_index is not None\n",
    "                        else (self.world.batch_dim, self.world.dim_p)\n",
    "                    ),\n",
    "                    device=self.world.device,\n",
    "                    dtype=torch.float32,\n",
    "                ).uniform_(\n",
    "                    -(self.bound - 0.1),\n",
    "                    self.bound - 0.1,\n",
    "                ),\n",
    "                batch_index=env_index,\n",
    "            )\n",
    "\n",
    "    def is_collision(self, agent1: Agent, agent2: Agent):\n",
    "        delta_pos = agent1.state.pos - agent2.state.pos\n",
    "        dist = torch.linalg.vector_norm(delta_pos, dim=-1)\n",
    "        dist_min = agent1.shape.radius + agent2.shape.radius\n",
    "        return dist < dist_min\n",
    "\n",
    "    # return all agents that are not adversaries\n",
    "    def good_agents(self):\n",
    "        return [agent for agent in self.world.agents if not agent.adversary]\n",
    "\n",
    "    # return all adversarial agents\n",
    "    def adversaries(self):\n",
    "        return [agent for agent in self.world.agents if agent.adversary]\n",
    "\n",
    "    def reward(self, agent: Agent):\n",
    "        is_first = agent == self.world.agents[0]\n",
    "\n",
    "        if is_first:\n",
    "            for a in self.world.agents:\n",
    "                a.rew = (\n",
    "                    self.adversary_reward(a) if a.adversary else self.agent_reward(a)\n",
    "                )\n",
    "            self.agents_rew = torch.stack(\n",
    "                [a.rew for a in self.good_agents()], dim=-1\n",
    "            ).sum(-1)\n",
    "            self.adverary_rew = torch.stack(\n",
    "                [a.rew for a in self.adversaries()], dim=-1\n",
    "            ).sum(-1)\n",
    "            if self.respawn_at_catch:\n",
    "                for a in self.good_agents():\n",
    "                    for adv in self.adversaries():\n",
    "                        coll = self.is_collision(a, adv)\n",
    "                        a.state.pos[coll] = torch.zeros(\n",
    "                            (self.world.batch_dim, self.world.dim_p),\n",
    "                            device=self.world.device,\n",
    "                            dtype=torch.float32,\n",
    "                        ).uniform_(-self.bound, self.bound,)[coll]\n",
    "                        a.state.vel[coll] = 0.0\n",
    "\n",
    "        if agent.adversary:\n",
    "            if self.adversaries_share_rew:\n",
    "                return self.adverary_rew\n",
    "            else:\n",
    "                return agent.rew\n",
    "        else:\n",
    "            if self.agents_share_rew:\n",
    "                return self.agents_rew\n",
    "            else:\n",
    "                return agent.rew\n",
    "\n",
    "    def agent_reward(self, agent: Agent):\n",
    "        # Agents are negatively rewarded if caught by adversaries\n",
    "        rew = torch.zeros(\n",
    "            self.world.batch_dim, device=self.world.device, dtype=torch.float32\n",
    "        )\n",
    "        adversaries = self.adversaries()\n",
    "        if self.shape_agent_rew:\n",
    "            # reward can optionally be shaped (increased reward for increased distance from adversary)\n",
    "            for adv in adversaries:\n",
    "                rew += 0.3 * torch.linalg.vector_norm(\n",
    "                    agent.state.pos - adv.state.pos, dim=-1\n",
    "                )\n",
    "        if agent.collide:\n",
    "            for a in adversaries:\n",
    "                rew[self.is_collision(a, agent)] -= 10\n",
    "\n",
    "        return rew\n",
    "\n",
    "    def adversary_reward(self, agent: Agent):\n",
    "        # Adversaries are rewarded for collisions with agents\n",
    "        rew = torch.zeros(\n",
    "            self.world.batch_dim, device=self.world.device, dtype=torch.float32\n",
    "        )\n",
    "        agents = self.good_agents()\n",
    "        if (\n",
    "            self.shape_adversary_rew\n",
    "        ):  # reward can optionally be shaped (decreased reward for increased distance from agents)\n",
    "            rew -= (\n",
    "                0.1\n",
    "                * torch.min(\n",
    "                    torch.stack(\n",
    "                        [\n",
    "                            torch.linalg.vector_norm(\n",
    "                                a.state.pos - agent.state.pos,\n",
    "                                dim=-1,\n",
    "                            )\n",
    "                            for a in agents\n",
    "                        ],\n",
    "                        dim=-1,\n",
    "                    ),\n",
    "                    dim=-1,\n",
    "                )[0]\n",
    "            )\n",
    "        if agent.collide:\n",
    "            for ag in agents:\n",
    "                rew[self.is_collision(ag, agent)] += 10\n",
    "        return rew\n",
    "\n",
    "    def observation(self, agent: Agent):\n",
    "        # get positions of all entities in this agent's reference frame\n",
    "        entity_pos = []\n",
    "        for entity in self.world.landmarks:\n",
    "            entity_pos.append(entity.state.pos - agent.state.pos)\n",
    "\n",
    "        other_pos = []\n",
    "        other_vel = []\n",
    "        for other in self.world.agents:\n",
    "            if other is agent:\n",
    "                continue\n",
    "            if agent.adversary and not other.adversary:\n",
    "                other_pos.append(other.state.pos - agent.state.pos)\n",
    "                other_vel.append(other.state.vel)\n",
    "            elif not agent.adversary and not other.adversary and self.observe_same_team:\n",
    "                other_pos.append(other.state.pos - agent.state.pos)\n",
    "                other_vel.append(other.state.vel)\n",
    "            elif not agent.adversary and other.adversary:\n",
    "                other_pos.append(other.state.pos - agent.state.pos)\n",
    "            elif agent.adversary and other.adversary and self.observe_same_team:\n",
    "                other_pos.append(other.state.pos - agent.state.pos)\n",
    "\n",
    "        return torch.cat(\n",
    "            [\n",
    "                *([agent.state.vel] if self.observe_vel else []),\n",
    "                *([agent.state.pos] if self.observe_pos else []),\n",
    "                *entity_pos,\n",
    "                *other_pos,\n",
    "                *other_vel,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "    def extra_render(self, env_index: int = 0):\n",
    "        from vmas.simulator import rendering\n",
    "\n",
    "        geoms = []\n",
    "\n",
    "        # Perimeter\n",
    "        for i in range(4):\n",
    "            geom = Line(\n",
    "                length=2\n",
    "                * ((self.bound - self.adversary_radius) + self.adversary_radius * 2)\n",
    "            ).get_geometry()\n",
    "            xform = rendering.Transform()\n",
    "            geom.add_attr(xform)\n",
    "\n",
    "            xform.set_translation(\n",
    "                (\n",
    "                    0.0\n",
    "                    if i % 2\n",
    "                    else (\n",
    "                        self.bound + self.adversary_radius\n",
    "                        if i == 0\n",
    "                        else -self.bound - self.adversary_radius\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    0.0\n",
    "                    if not i % 2\n",
    "                    else (\n",
    "                        self.bound + self.adversary_radius\n",
    "                        if i == 1\n",
    "                        else -self.bound - self.adversary_radius\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "            xform.set_rotation(torch.pi / 2 if not i % 2 else 0.0)\n",
    "            color = Color.BLACK.value\n",
    "            if isinstance(color, torch.Tensor) and len(color.shape) > 1:\n",
    "                color = color[env_index]\n",
    "            geom.set_color(*color)\n",
    "            geoms.append(geom)\n",
    "        return geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363c76aa-8cb4-4e33-9457-d29d6733dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100  # Environment steps before done\n",
    "\n",
    "n_chasers = 1\n",
    "n_evaders = 10\n",
    "n_obstacles = 2\n",
    "\n",
    "num_vmas_envs = (\n",
    "    frames_per_batch // max_steps\n",
    ")  # Number of vectorized environments. frames_per_batch collection will be divided among these environments\n",
    "base_env = VmasEnv(\n",
    "    scenario=PredationScenario(),\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=True,\n",
    "    max_steps=max_steps,\n",
    "    device=device,\n",
    "    seed=seed,\n",
    "    # Scenario specific\n",
    "    num_good_agents=n_evaders,\n",
    "    num_adversaries=n_chasers,\n",
    "    num_landmarks=n_obstacles,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "297b74c2-c030-480b-aa3b-0d6f97a19846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_map: {'agents': ['agent_0', 'agent_1', 'agent_2', 'agent_3', 'agent_4', 'agent_5', 'agent_6', 'agent_7', 'agent_8', 'agent_9']}\n"
     ]
    }
   ],
   "source": [
    "print(f\"group_map: {base_env.group_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2e2a88-8d6f-49e6-b4c2-86b5cb64377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: Composite(\n",
      "    agents: Composite(\n",
      "        action: BoundedContinuous(\n",
      "            shape=torch.Size([10, 10, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 10, 2]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 10, 2]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 10])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([10]))\n",
      "reward_spec: Composite(\n",
      "    agents: Composite(\n",
      "        reward: UnboundedContinuous(\n",
      "            shape=torch.Size([10, 10, 1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 10, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 10, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 10])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([10]))\n",
      "done_spec: Composite(\n",
      "    done: Categorical(\n",
      "        shape=torch.Size([10, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: Categorical(\n",
      "        shape=torch.Size([10, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([10]))\n",
      "observation_spec: Composite(\n",
      "    agents: Composite(\n",
      "        observation: UnboundedContinuous(\n",
      "            shape=torch.Size([10, 10, 44]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 10, 44]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 10, 44]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 10])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([10]))\n"
     ]
    }
   ],
   "source": [
    "print(\"action_spec:\", base_env.full_action_spec)\n",
    "print(\"reward_spec:\", base_env.full_reward_spec)\n",
    "print(\"done_spec:\", base_env.full_done_spec)\n",
    "print(\"observation_spec:\", base_env.observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22835b86-67a4-45bd-808b-3c0822fcf856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_keys: [('agents', 'action')]\n",
      "reward_keys: [('agents', 'reward')]\n",
      "done_keys: ['done', 'terminated']\n"
     ]
    }
   ],
   "source": [
    "print(\"action_keys:\", base_env.action_keys)\n",
    "print(\"reward_keys:\", base_env.reward_keys)\n",
    "print(\"done_keys:\", base_env.done_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c6c92-9f55-4698-b134-bcdaa6b73679",
   "metadata": {},
   "source": [
    "## ENVIRONMENT TRANSFORMATION\n",
    "Transforms the environment so that it computes the sum of the reward and resets it when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d054c360-bcb0-4adb-8be5-ef4841802722",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    RewardSum(\n",
    "        in_keys=base_env.reward_keys,\n",
    "        reset_keys=[\"_reset\"] * len(base_env.group_map.keys()),\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc091e9-e4a8-43cc-8cb6-a8a114d85b37",
   "metadata": {},
   "source": [
    "## ENVIRONMENT SPECS TEST\n",
    "Makes sure that the environment fulfills the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53ebef1-cbcf-46c8-b440-194424962ef3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcheck_env_specs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torchrl/envs/utils.py:733\u001b[39m, in \u001b[36mcheck_env_specs\u001b[39m\u001b[34m(env, return_contiguous, check_dtype, seed, tensordict)\u001b[39m\n\u001b[32m    731\u001b[39m     fake_tensordict = fake_tensordict.expand(shape)\n\u001b[32m    732\u001b[39m     tensordict = tensordict.expand(shape)\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m real_tensordict = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_contiguous\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_contiguous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_reset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_contiguous:\n\u001b[32m    741\u001b[39m     fake_tensordict = fake_tensordict.unsqueeze(real_tensordict.batch_dims - \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torchrl/envs/common.py:3131\u001b[39m, in \u001b[36mEnvBase.rollout\u001b[39m\u001b[34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, break_when_all_done, return_contiguous, tensordict, set_truncated, out, trust_policy)\u001b[39m\n\u001b[32m   3121\u001b[39m kwargs = {\n\u001b[32m   3122\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtensordict\u001b[39m\u001b[33m\"\u001b[39m: tensordict,\n\u001b[32m   3123\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mauto_cast_to_device\u001b[39m\u001b[33m\"\u001b[39m: auto_cast_to_device,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3128\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallback\u001b[39m\u001b[33m\"\u001b[39m: callback,\n\u001b[32m   3129\u001b[39m }\n\u001b[32m   3130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m break_when_any_done \u001b[38;5;129;01mor\u001b[39;00m break_when_all_done:\n\u001b[32m-> \u001b[39m\u001b[32m3131\u001b[39m     tensordicts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rollout_stop_early\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbreak_when_all_done\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbreak_when_all_done\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbreak_when_any_done\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbreak_when_any_done\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3134\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3135\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3136\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3137\u001b[39m     tensordicts = \u001b[38;5;28mself\u001b[39m._rollout_nonstop(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torchrl/envs/common.py:3272\u001b[39m, in \u001b[36mEnvBase._rollout_stop_early\u001b[39m\u001b[34m(self, break_when_any_done, break_when_all_done, tensordict, auto_cast_to_device, max_steps, policy, policy_device, env_device, callback)\u001b[39m\n\u001b[32m   3270\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3271\u001b[39m         tensordict.clear_device_()\n\u001b[32m-> \u001b[39m\u001b[32m3272\u001b[39m tensordict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3273\u001b[39m td_append = tensordict.copy()\n\u001b[32m   3274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m break_when_all_done:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torchrl/envs/common.py:1973\u001b[39m, in \u001b[36mEnvBase.step\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m   1969\u001b[39m     tensordict_batch_size = \u001b[38;5;28mself\u001b[39m.batch_size\n\u001b[32m   1971\u001b[39m next_preset = tensordict.get(\u001b[33m\"\u001b[39m\u001b[33mnext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1973\u001b[39m next_tensordict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1974\u001b[39m next_tensordict = \u001b[38;5;28mself\u001b[39m._step_proc_data(next_tensordict)\n\u001b[32m   1975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1976\u001b[39m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[32m   1977\u001b[39m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[32m   1978\u001b[39m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torchrl/envs/transforms/transforms.py:926\u001b[39m, in \u001b[36mTransformedEnv._step\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m    924\u001b[39m next_preset = tensordict.get(\u001b[33m\"\u001b[39m\u001b[33mnext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    925\u001b[39m tensordict_in = \u001b[38;5;28mself\u001b[39m.transform.inv(tensordict)\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m next_tensordict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    928\u001b[39m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[32m    929\u001b[39m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[32m    930\u001b[39m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n\u001b[32m    931\u001b[39m     next_tensordict.update(\n\u001b[32m    932\u001b[39m         next_preset.exclude(*next_tensordict.keys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m    933\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torchrl/envs/libs/vmas.py:572\u001b[39m, in \u001b[36mVmasWrapper._step\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m    569\u001b[39m     action_list += group_action_list\n\u001b[32m    570\u001b[39m action = [action_list[agent_indices[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.n_agents)]\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m obs, rews, dones, infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m dones = \u001b[38;5;28mself\u001b[39m.read_done(dones)\n\u001b[32m    576\u001b[39m source = {\u001b[33m\"\u001b[39m\u001b[33mdone\u001b[39m\u001b[33m\"\u001b[39m: dones, \u001b[33m\"\u001b[39m\u001b[33mterminated\u001b[39m\u001b[33m\"\u001b[39m: dones.clone()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/vmas/simulator/environment/environment.py:272\u001b[39m, in \u001b[36mEnvironment.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28mself\u001b[39m.scenario.post_step()\n\u001b[32m    270\u001b[39m \u001b[38;5;28mself\u001b[39m.steps += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_from_scenario\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_observations\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_infos\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_rewards\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_dones\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/vmas/simulator/environment/environment.py:157\u001b[39m, in \u001b[36mEnvironment.get_from_scenario\u001b[39m\u001b[34m(self, get_observations, get_rewards, get_infos, get_dones, dict_agent_names)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_rewards:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agents:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         reward = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m.clone()\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m dict_agent_names:\n\u001b[32m    159\u001b[39m             rewards.update({agent.name: reward})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mPredationScenario.reward\u001b[39m\u001b[34m(self, agent)\u001b[39m\n\u001b[32m    111\u001b[39m     a.rew = (\n\u001b[32m    112\u001b[39m         \u001b[38;5;28mself\u001b[39m.adversary_reward(a) \u001b[38;5;28;01mif\u001b[39;00m a.adversary \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agent_reward(a)\n\u001b[32m    113\u001b[39m     )\n\u001b[32m    114\u001b[39m \u001b[38;5;28mself\u001b[39m.agents_rew = torch.stack(\n\u001b[32m    115\u001b[39m     [a.rew \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.good_agents()], dim=-\u001b[32m1\u001b[39m\n\u001b[32m    116\u001b[39m ).sum(-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28mself\u001b[39m.adverary_rew = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrew\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madversaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    119\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.sum(-\u001b[32m1\u001b[39m)\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.respawn_at_catch:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.good_agents():\n",
      "\u001b[31mRuntimeError\u001b[39m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3cbb7-4514-4ab9-9b35-a60910e7beb7",
   "metadata": {},
   "source": [
    "# TEST ROLLOUT\n",
    "Tests the environment specification by performing a small dummy rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42434206-38d4-4420-8abb-8ea036bff532",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rollout_steps = 5\n",
    "rollout = env.rollout(n_rollout_steps)\n",
    "print(f\"rollout of {n_rollout_steps} steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f127c2-169c-45bc-a6d9-69cc05444599",
   "metadata": {},
   "source": [
    "# POLICY\n",
    "Creates the policy module for each agent group and sets up a policy for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff4d15-3c18-4f9f-b4af-8cbf4a36eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_modules = {}\n",
    "for group, agents in env.group_map.items():\n",
    "    share_parameters_policy = True  # Can change this based on the group\n",
    "\n",
    "    policy_net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[group, \"observation\"].shape[\n",
    "            -1\n",
    "        ],  # n_obs_per_agent\n",
    "        n_agent_outputs=env.full_action_spec[group, \"action\"].shape[\n",
    "            -1\n",
    "        ],  # n_actions_per_agents\n",
    "        n_agents=len(agents),  # Number of agents in the group\n",
    "        centralised=False,  # the policies are decentralised (i.e., each agent will act from its local observation)\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    )\n",
    "\n",
    "    # Wrap the neural network in a :class:`~tensordict.nn.TensorDictModule`.\n",
    "    # This is simply a module that will read the ``in_keys`` from a tensordict, feed them to the\n",
    "    # neural networks, and write the\n",
    "    # outputs in-place at the ``out_keys``.\n",
    "\n",
    "    policy_module = TensorDictModule(\n",
    "        policy_net,\n",
    "        in_keys=[(group, \"observation\")],\n",
    "        out_keys=[(group, \"param\")],\n",
    "    )  # We just name the input and output that the network will read and write to the input tensordict\n",
    "    policy_modules[group] = policy_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c77f5-c50d-4027-a2de-bce3c558720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    policy = ProbabilisticActor(\n",
    "        module=policy_modules[group],\n",
    "        spec=env.full_action_spec[group, \"action\"],\n",
    "        in_keys=[(group, \"param\")],\n",
    "        out_keys=[(group, \"action\")],\n",
    "        distribution_class=TanhDelta,\n",
    "        distribution_kwargs={\n",
    "            \"low\": env.full_action_spec_unbatched[group, \"action\"].space.low,\n",
    "            \"high\": env.full_action_spec_unbatched[group, \"action\"].space.high,\n",
    "        },\n",
    "        return_log_prob=False,\n",
    "    )\n",
    "    policies[group] = policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e322ded-94f4-4f0a-bcab-572d0c930721",
   "metadata": {},
   "source": [
    "# EXPLORATION POLICIES\n",
    "Creates the exploration policies because DDMPG is deterministic and we still want to include some exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b1681-7a93-4861-93cb-7e31d2c9043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_policies = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    exploration_policy = TensorDictSequential(\n",
    "        policies[group],\n",
    "        AdditiveGaussianModule(\n",
    "            spec=policies[group].spec,\n",
    "            annealing_num_steps=total_frames\n",
    "            // 2,  # Number of frames after which sigma is sigma_end\n",
    "            action_key=(group, \"action\"),\n",
    "            sigma_init=0.9,  # Initial value of the sigma\n",
    "            sigma_end=0.1,  # Final value of the sigma\n",
    "        ),\n",
    "    )\n",
    "    exploration_policies[group] = exploration_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363c165-8c1f-4973-84ac-c7ad3cd352db",
   "metadata": {},
   "source": [
    "# CRITICS\n",
    "Creates the critics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b218217-1a9d-4b19-8a3c-738ec2d5bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "critics = {}\n",
    "for group, agents in env.group_map.items():\n",
    "    share_parameters_critic = True  # Can change for each group\n",
    "    MADDPG = True  # IDDPG if False, can change for each group\n",
    "\n",
    "    # This module applies the lambda function: reading the action and observation entries for the group\n",
    "    # and concatenating them in a new ``(group, \"obs_action\")`` entry\n",
    "    cat_module = TensorDictModule(\n",
    "        lambda obs, action: torch.cat([obs, action], dim=-1),\n",
    "        in_keys=[(group, \"observation\"), (group, \"action\")],\n",
    "        out_keys=[(group, \"obs_action\")],\n",
    "    )\n",
    "\n",
    "    critic_module = TensorDictModule(\n",
    "        module=MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[group, \"observation\"].shape[-1]\n",
    "            + env.full_action_spec[group, \"action\"].shape[-1],\n",
    "            n_agent_outputs=1,  # 1 value per agent\n",
    "            n_agents=len(agents),\n",
    "            centralised=MADDPG,\n",
    "            share_params=share_parameters_critic,\n",
    "            device=device,\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=torch.nn.Tanh,\n",
    "        ),\n",
    "        in_keys=[(group, \"obs_action\")],  # Read ``(group, \"obs_action\")``\n",
    "        out_keys=[\n",
    "            (group, \"state_action_value\")\n",
    "        ],  # Write ``(group, \"state_action_value\")``\n",
    "    )\n",
    "\n",
    "    critics[group] = TensorDictSequential(\n",
    "        cat_module, critic_module\n",
    "    )  # Run them in sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f3ede-27fa-48a6-af12-97d7f9c4c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_td = env.reset()\n",
    "for group, _agents in env.group_map.items():\n",
    "    print(\n",
    "        f\"Running value and policy for group '{group}':\",\n",
    "        critics[group](policies[group](reset_td)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff98a3-2ae5-4ff7-91fc-b3322d4f4fb2",
   "metadata": {},
   "source": [
    "# COLLECTOR\n",
    "Creates the data collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9cbad-fb79-4d5d-8c41-b44461f9097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put exploration policies from each group in a sequence\n",
    "agents_exploration_policy = TensorDictSequential(*exploration_policies.values())\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    agents_exploration_policy,\n",
    "    device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e03bd1-fb48-4d4e-a2c2-37839a8bcde2",
   "metadata": {},
   "source": [
    "# REPLAY BUFFERS\n",
    "Creates the replay buffers for the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9d98d-ca35-406c-bddb-e55d2addeaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffers = {}\n",
    "scratch_dirs = []\n",
    "for group, _agents in env.group_map.items():\n",
    "    scratch_dir = tempfile.TemporaryDirectory().name\n",
    "    scratch_dirs.append(scratch_dir)\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        storage=LazyMemmapStorage(\n",
    "            memory_size,\n",
    "            scratch_dir=scratch_dir,\n",
    "        ),  # We will store up to memory_size multi-agent transitions\n",
    "        sampler=RandomSampler(),\n",
    "        batch_size=train_batch_size,  # We will sample batches of this size\n",
    "    )\n",
    "    if device.type != \"cpu\":\n",
    "        replay_buffer.append_transform(lambda x: x.to(device))\n",
    "    replay_buffers[group] = replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e947ba7-0d36-4646-bbd0-61e5c470dfc3",
   "metadata": {},
   "source": [
    "# OPTIMIZERS\n",
    "sets up the loss functions and optimizers for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6a8b4-fede-4e70-ab2f-476000b9ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    loss_module = DDPGLoss(\n",
    "        actor_network=policies[group],  # Use the non-explorative policies\n",
    "        value_network=critics[group],\n",
    "        delay_value=True,  # Whether to use a target network for the value\n",
    "        loss_function=\"l2\",\n",
    "    )\n",
    "    loss_module.set_keys(\n",
    "        state_action_value=(group, \"state_action_value\"),\n",
    "        reward=(group, \"reward\"),\n",
    "        done=(group, \"done\"),\n",
    "        terminated=(group, \"terminated\"),\n",
    "    )\n",
    "    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
    "\n",
    "    losses[group] = loss_module\n",
    "\n",
    "target_updaters = {\n",
    "    group: SoftUpdate(loss, tau=polyak_tau) for group, loss in losses.items()\n",
    "}\n",
    "\n",
    "optimisers = {\n",
    "    group: {\n",
    "        \"loss_actor\": torch.optim.Adam(\n",
    "            loss.actor_network_params.flatten_keys().values(), lr=lr\n",
    "        ),\n",
    "        \"loss_value\": torch.optim.Adam(\n",
    "            loss.value_network_params.flatten_keys().values(), lr=lr\n",
    "        ),\n",
    "    }\n",
    "    for group, loss in losses.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2ca9c-a29b-4535-bcc5-4158daa25669",
   "metadata": {},
   "source": [
    "# HELPER METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea3024-601e-4289-8fd0-748659c567c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch: TensorDictBase) -> TensorDictBase:\n",
    "    \"\"\"\n",
    "    If the `(group, \"terminated\")` and `(group, \"done\")` keys are not present, create them by expanding\n",
    "    `\"terminated\"` and `\"done\"`.\n",
    "    This is needed to present them with the same shape as the reward to the loss.\n",
    "    \"\"\"\n",
    "    for group in env.group_map.keys():\n",
    "        keys = list(batch.keys(True, True))\n",
    "        group_shape = batch.get_item_shape(group)\n",
    "        nested_done_key = (\"next\", group, \"done\")\n",
    "        nested_terminated_key = (\"next\", group, \"terminated\")\n",
    "        if nested_done_key not in keys:\n",
    "            batch.set(\n",
    "                nested_done_key,\n",
    "                batch.get((\"next\", \"done\")).unsqueeze(-1).expand((*group_shape, 1)),\n",
    "            )\n",
    "        if nested_terminated_key not in keys:\n",
    "            batch.set(\n",
    "                nested_terminated_key,\n",
    "                batch.get((\"next\", \"terminated\"))\n",
    "                .unsqueeze(-1)\n",
    "                .expand((*group_shape, 1)),\n",
    "            )\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30960af0-d33a-4c23-a643-581df40f0734",
   "metadata": {},
   "source": [
    "# TRAINING\n",
    "Trains the two groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144c102-54da-489a-bed4-b9d092295825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(\n",
    "    total=n_iters,\n",
    "    desc=\", \".join(\n",
    "        [f\"episode_reward_mean_{group} = 0\" for group in env.group_map.keys()]\n",
    "    ),\n",
    ")\n",
    "episode_reward_mean_map = {group: [] for group in env.group_map.keys()}\n",
    "train_group_map = copy.deepcopy(env.group_map)\n",
    "\n",
    "# Training/collection iterations\n",
    "for iteration, batch in enumerate(collector):\n",
    "    current_frames = batch.numel()\n",
    "    batch = process_batch(batch)  # Util to expand done keys if needed\n",
    "    # Loop over groups\n",
    "    for group in train_group_map.keys():\n",
    "        group_batch = batch.exclude(\n",
    "            *[\n",
    "                key\n",
    "                for _group in env.group_map.keys()\n",
    "                if _group != group\n",
    "                for key in [_group, (\"next\", _group)]\n",
    "            ]\n",
    "        )  # Exclude data from other groups\n",
    "        group_batch = group_batch.reshape(\n",
    "            -1\n",
    "        )  # This just affects the leading dimensions in batch_size of the tensordict\n",
    "        replay_buffers[group].extend(group_batch)\n",
    "\n",
    "        for _ in range(n_optimiser_steps):\n",
    "            subdata = replay_buffers[group].sample()\n",
    "            loss_vals = losses[group](subdata)\n",
    "\n",
    "            for loss_name in [\"loss_actor\", \"loss_value\"]:\n",
    "                loss = loss_vals[loss_name]\n",
    "                optimiser = optimisers[group][loss_name]\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                # Optional\n",
    "                params = optimiser.param_groups[0][\"params\"]\n",
    "                torch.nn.utils.clip_grad_norm_(params, max_grad_norm)\n",
    "\n",
    "                optimiser.step()\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "            # Soft-update the target network\n",
    "            target_updaters[group].step()\n",
    "\n",
    "        # Exploration sigma anneal update\n",
    "        exploration_policies[group][-1].step(current_frames)\n",
    "\n",
    "    # Stop training a certain group when a condition is met (e.g., number of training iterations)\n",
    "    if iteration == iteration_when_stop_training_evaders:\n",
    "        del train_group_map[\"agent\"]\n",
    "\n",
    "    # Logging\n",
    "    for group in env.group_map.keys():\n",
    "        episode_reward_mean = (\n",
    "            batch.get((\"next\", group, \"episode_reward\"))[\n",
    "                batch.get((\"next\", group, \"done\"))\n",
    "            ]\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        episode_reward_mean_map[group].append(episode_reward_mean)\n",
    "\n",
    "    pbar.set_description(\n",
    "        \", \".join(\n",
    "            [\n",
    "                f\"episode_reward_mean_{group} = {episode_reward_mean_map[group][-1]}\"\n",
    "                for group in env.group_map.keys()\n",
    "            ]\n",
    "        ),\n",
    "        refresh=False,\n",
    "    )\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126e664-ea46-432e-bda5-174383890490",
   "metadata": {},
   "source": [
    "# VISUALISATION\n",
    "Visualizes the mean reward per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6f83b-557a-413d-92a6-c37f275ae9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "for i, group in enumerate(env.group_map.keys()):\n",
    "    axs[i].plot(episode_reward_mean_map[group], label=f\"Episode reward mean {group}\")\n",
    "    axs[i].set_ylabel(\"Reward\")\n",
    "    axs[i].axvline(\n",
    "        x=iteration_when_stop_training_evaders,\n",
    "        label=\"Agent (evader) stop training\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    axs[i].legend()\n",
    "axs[-1].set_xlabel(\"Training iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794ef34-856c-4a80-9e08-d74ce35a60ed",
   "metadata": {},
   "source": [
    "## Video\n",
    "Creates and saves a video replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e62e86-5743-4857-97a7-6c7127963964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace tmpdir with any desired path where the video should be saved\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    print(f\"tmp: {tmpdir}\")\n",
    "    tmpdir = \"/home/lilly/dev/marl-predator-prey/marl-predator-prey/videos\"\n",
    "    video_logger = CSVLogger(\"vmas_logs\", tmpdir, video_format=\"mp4\")\n",
    "    print(\"Creating rendering env\")\n",
    "    env_with_render = TransformedEnv(env.base_env, env.transform.clone())\n",
    "    env_with_render = env_with_render.append_transform(\n",
    "        PixelRenderTransform(\n",
    "            out_keys=[\"pixels\"],\n",
    "            # the np.ndarray has a negative stride and needs to be copied before being cast to a tensor\n",
    "            preproc=lambda x: x.copy(),\n",
    "            as_non_tensor=True,\n",
    "            # asking for array rather than on-screen rendering\n",
    "            mode=\"rgb_array\",\n",
    "        )\n",
    "    )\n",
    "    env_with_render = env_with_render.append_transform(\n",
    "        VideoRecorder(logger=video_logger, tag=\"vmas_rendered\")\n",
    "    )\n",
    "    with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
    "        print(\"Rendering rollout...\")\n",
    "        env_with_render.rollout(100, policy=agents_exploration_policy)\n",
    "    print(\"Saving the video...\")\n",
    "    env_with_render.transform.dump()\n",
    "    print(\"Saved! Saved directory tree:\")\n",
    "    video_logger.print_log_dir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18b25d-eb45-4990-95dc-b98e828feed5",
   "metadata": {},
   "source": [
    "# CLEANUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4600cf-19b1-4edc-a591-e612e2b37344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove scratch dir\n",
    "try:\n",
    "    import shutil\n",
    "\n",
    "    for scratch_dir in scratch_dirs:\n",
    "        # Use shutil.rmtree() to delete the directory and all its contents\n",
    "        shutil.rmtree(scratch_dir)\n",
    "        print(f\"Directory '{scratch_dir}' deleted successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory '{scratch_dir}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting directory: {e}\")\n",
    "# sphinx_gallery_end_ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d346936-fa20-490d-8010-2b3d54372a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
