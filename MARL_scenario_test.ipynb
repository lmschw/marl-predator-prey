{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1201361e-326f-4a3c-bfec-287bb6dd09da",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-tutorials/multiagent_competitive_ddpg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315b3bb-d5c2-4b6d-be8e-ed2680ba47b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d791be-3c65-4557-b40f-33032a13e03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lilly/dev/marl-predator-prey/marl-predator-prey/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "\n",
    "from vmas import render_interactively\n",
    "from vmas.simulator.core import Agent, Landmark, Line, Sphere, World\n",
    "from vmas.simulator.scenario import BaseScenario\n",
    "from vmas.simulator.utils import Color, ScenarioUtils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensordict import TensorDictBase\n",
    "\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch import multiprocessing\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyMemmapStorage, RandomSampler, ReplayBuffer\n",
    "\n",
    "from torchrl.envs import (\n",
    "    check_env_specs,\n",
    "    ExplorationType,\n",
    "    PettingZooEnv,\n",
    "    RewardSum,\n",
    "    set_exploration_type,\n",
    "    TransformedEnv,\n",
    "    VmasEnv,\n",
    ")\n",
    "\n",
    "from torchrl.modules import (\n",
    "    AdditiveGaussianModule,\n",
    "    MultiAgentMLP,\n",
    "    ProbabilisticActor,\n",
    "    TanhDelta,\n",
    ")\n",
    "\n",
    "from torchrl.objectives import DDPGLoss, SoftUpdate, ValueEstimators\n",
    "\n",
    "from torchrl.record import CSVLogger, PixelRenderTransform, VideoRecorder\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c4391-73e4-4406-8693-a890d5f46725",
   "metadata": {},
   "source": [
    "# GENERAL\n",
    "Setting up of constants such as the seed, the device and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dac0bc9-9d98-43c3-bf57-e3630e5530cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Devices\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 1000 # Number of team frames collected per sampling iteration\n",
    "n_iters = 100  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "\n",
    "# Replay buffer\n",
    "memory_size = 1_000_000  # The replay buffer of each group can store this many frames\n",
    "\n",
    "# Training\n",
    "n_optimiser_steps = 100  # Number of optimization steps per training iteration\n",
    "train_batch_size = 128  # Number of frames trained in each optimiser step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "\n",
    "# DDPG\n",
    "gamma = 0.99  # Discount factor\n",
    "polyak_tau = 0.005  # Tau for the soft-update of the target network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fdfff9-e738-4f42-84e5-01ac57693e84",
   "metadata": {},
   "source": [
    "# CREATING THE ENVIRONMENT\n",
    " Creates a standard simple_tag environment with the specified number of chasers and evaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045d86a0-a417-496c-8a56-5b4bd1892cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadMovementScenario(BaseScenario):\n",
    "    def make_world(self, batch_dim: int, device: torch.device, **kwargs):\n",
    "        num_agents = kwargs.pop(\"num_agents\", 1)\n",
    "        num_landmarks = kwargs.pop(\"num_landmarks\", 2)\n",
    "        self.shape_agent_rew = kwargs.pop(\"shape_agent_rew\", False)\n",
    "        self.agents_share_rew = kwargs.pop(\"agents_share_rew\", False)\n",
    "        self.observe_same_team = kwargs.pop(\"observe_same_team\", True)\n",
    "        self.observe_pos = kwargs.pop(\"observe_pos\", True)\n",
    "        self.observe_vel = kwargs.pop(\"observe_vel\", True)\n",
    "        self.bound = kwargs.pop(\"bound\", 1.0)\n",
    "        self.respawn_at_catch = kwargs.pop(\"respawn_at_catch\", False)\n",
    "        ScenarioUtils.check_kwargs_consumed(kwargs)\n",
    "\n",
    "        self.visualize_semidims = False\n",
    "\n",
    "        world = World(\n",
    "            batch_dim=batch_dim,\n",
    "            device=device,\n",
    "            x_semidim=self.bound,\n",
    "            y_semidim=self.bound,\n",
    "            substeps=10,\n",
    "            collision_force=500,\n",
    "        )\n",
    "\n",
    "        # Add agents\n",
    "        for i in range(num_agents):\n",
    "            name = f\"agent_{i}\"\n",
    "            agent = Agent(\n",
    "                name=name,\n",
    "                collide=True,\n",
    "                shape=Sphere(radius=self.adversary_radius if adversary else 0.05),\n",
    "                u_multiplier=3.0 if adversary else 4.0,\n",
    "                max_speed=1.0 if adversary else 1.3,\n",
    "                color=Color.RED if adversary else Color.GREEN,\n",
    "            )\n",
    "            world.add_agent(agent)\n",
    "        # Add landmarks\n",
    "        for i in range(num_landmarks):\n",
    "            landmark = Landmark(\n",
    "                name=f\"landmark {i}\",\n",
    "                collide=True,\n",
    "                shape=Sphere(radius=0.2),\n",
    "                color=Color.BLACK,\n",
    "            )\n",
    "            world.add_landmark(landmark)\n",
    "\n",
    "        return world\n",
    "\n",
    "    def reset_world_at(self, env_index: int = None):\n",
    "        for agent in self.world.agents:\n",
    "            agent.set_pos(\n",
    "                torch.zeros(\n",
    "                    (\n",
    "                        (1, self.world.dim_p)\n",
    "                        if env_index is not None\n",
    "                        else (self.world.batch_dim, self.world.dim_p)\n",
    "                    ),\n",
    "                    device=self.world.device,\n",
    "                    dtype=torch.float32,\n",
    "                ).uniform_(\n",
    "                    -self.bound,\n",
    "                    self.bound,\n",
    "                ),\n",
    "                batch_index=env_index,\n",
    "            )\n",
    "\n",
    "        for landmark in self.world.landmarks:\n",
    "            landmark.set_pos(\n",
    "                torch.zeros(\n",
    "                    (\n",
    "                        (1, self.world.dim_p)\n",
    "                        if env_index is not None\n",
    "                        else (self.world.batch_dim, self.world.dim_p)\n",
    "                    ),\n",
    "                    device=self.world.device,\n",
    "                    dtype=torch.float32,\n",
    "                ).uniform_(\n",
    "                    -(self.bound - 0.1),\n",
    "                    self.bound - 0.1,\n",
    "                ),\n",
    "                batch_index=env_index,\n",
    "            )\n",
    "\n",
    "    def is_collision(self, agent1: Agent, agent2: Agent):\n",
    "        delta_pos = agent1.state.pos - agent2.state.pos\n",
    "        dist = torch.linalg.vector_norm(delta_pos, dim=-1)\n",
    "        dist_min = agent1.shape.radius + agent2.shape.radius\n",
    "        return dist < dist_min\n",
    "\n",
    "    # return all agents that are not adversaries\n",
    "    def good_agents(self):\n",
    "        return [agent for agent in self.world.agents if not agent.adversary]\n",
    "\n",
    "    # return all adversarial agents\n",
    "    def adversaries(self):\n",
    "        return [agent for agent in self.world.agents if agent.adversary]\n",
    "\n",
    "    def reward(self, agent: Agent):\n",
    "        is_first = agent == self.world.agents[0]\n",
    "\n",
    "        if is_first:\n",
    "            for a in self.world.agents:\n",
    "                a.rew = (\n",
    "                    self.adversary_reward(a) if a.adversary else self.agent_reward(a)\n",
    "                )\n",
    "            self.agents_rew = torch.stack(\n",
    "                [a.rew for a in self.good_agents()], dim=-1\n",
    "            ).sum(-1)\n",
    "            self.adverary_rew = torch.stack(\n",
    "                [a.rew for a in self.adversaries()], dim=-1\n",
    "            ).sum(-1)\n",
    "            if self.respawn_at_catch:\n",
    "                for a in self.good_agents():\n",
    "                    for adv in self.adversaries():\n",
    "                        coll = self.is_collision(a, adv)\n",
    "                        a.state.pos[coll] = torch.zeros(\n",
    "                            (self.world.batch_dim, self.world.dim_p),\n",
    "                            device=self.world.device,\n",
    "                            dtype=torch.float32,\n",
    "                        ).uniform_(-self.bound, self.bound,)[coll]\n",
    "                        a.state.vel[coll] = 0.0\n",
    "\n",
    "        if agent.adversary:\n",
    "            if self.adversaries_share_rew:\n",
    "                return self.adverary_rew\n",
    "            else:\n",
    "                return agent.rew\n",
    "        else:\n",
    "            if self.agents_share_rew:\n",
    "                return self.agents_rew\n",
    "            else:\n",
    "                return agent.rew\n",
    "\n",
    "    def agent_reward(self, agent: Agent):\n",
    "        # Agents are negatively rewarded if caught by adversaries\n",
    "        rew = torch.zeros(\n",
    "            self.world.batch_dim, device=self.world.device, dtype=torch.float32\n",
    "        )\n",
    "        adversaries = self.adversaries()\n",
    "        if self.shape_agent_rew:\n",
    "            # reward can optionally be shaped (increased reward for increased distance from adversary)\n",
    "            for adv in adversaries:\n",
    "                rew += 0.3 * torch.linalg.vector_norm(\n",
    "                    agent.state.pos - adv.state.pos, dim=-1\n",
    "                )\n",
    "        if agent.collide:\n",
    "            for a in adversaries:\n",
    "                rew[self.is_collision(a, agent)] -= 10\n",
    "\n",
    "        return rew\n",
    "\n",
    "    def adversary_reward(self, agent: Agent):\n",
    "        # Adversaries are rewarded for collisions with agents\n",
    "        rew = torch.zeros(\n",
    "            self.world.batch_dim, device=self.world.device, dtype=torch.float32\n",
    "        )\n",
    "        agents = self.good_agents()\n",
    "        if (\n",
    "            self.shape_adversary_rew\n",
    "        ):  # reward can optionally be shaped (decreased reward for increased distance from agents)\n",
    "            rew -= (\n",
    "                0.1\n",
    "                * torch.min(\n",
    "                    torch.stack(\n",
    "                        [\n",
    "                            torch.linalg.vector_norm(\n",
    "                                a.state.pos - agent.state.pos,\n",
    "                                dim=-1,\n",
    "                            )\n",
    "                            for a in agents\n",
    "                        ],\n",
    "                        dim=-1,\n",
    "                    ),\n",
    "                    dim=-1,\n",
    "                )[0]\n",
    "            )\n",
    "        if agent.collide:\n",
    "            for ag in agents:\n",
    "                rew[self.is_collision(ag, agent)] += 10\n",
    "        return rew\n",
    "\n",
    "    def observation(self, agent: Agent):\n",
    "        # get positions of all entities in this agent's reference frame\n",
    "        entity_pos = []\n",
    "        for entity in self.world.landmarks:\n",
    "            entity_pos.append(entity.state.pos - agent.state.pos)\n",
    "\n",
    "        other_pos = []\n",
    "        other_vel = []\n",
    "        for other in self.world.agents:\n",
    "            if other is agent:\n",
    "                continue\n",
    "            if agent.adversary and not other.adversary:\n",
    "                other_pos.append(other.state.pos - agent.state.pos)\n",
    "                other_vel.append(other.state.vel)\n",
    "            elif not agent.adversary and not other.adversary and self.observe_same_team:\n",
    "                other_pos.append(other.state.pos - agent.state.pos)\n",
    "                other_vel.append(other.state.vel)\n",
    "            elif not agent.adversary and other.adversary:\n",
    "                other_pos.append(other.state.pos - agent.state.pos)\n",
    "            elif agent.adversary and other.adversary and self.observe_same_team:\n",
    "                other_pos.append(other.state.pos - agent.state.pos)\n",
    "\n",
    "        return torch.cat(\n",
    "            [\n",
    "                *([agent.state.vel] if self.observe_vel else []),\n",
    "                *([agent.state.pos] if self.observe_pos else []),\n",
    "                *entity_pos,\n",
    "                *other_pos,\n",
    "                *other_vel,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "    def extra_render(self, env_index: int = 0):\n",
    "        from vmas.simulator import rendering\n",
    "\n",
    "        geoms = []\n",
    "\n",
    "        # Perimeter\n",
    "        for i in range(4):\n",
    "            geom = Line(\n",
    "                length=2\n",
    "                * ((self.bound - self.adversary_radius) + self.adversary_radius * 2)\n",
    "            ).get_geometry()\n",
    "            xform = rendering.Transform()\n",
    "            geom.add_attr(xform)\n",
    "\n",
    "            xform.set_translation(\n",
    "                (\n",
    "                    0.0\n",
    "                    if i % 2\n",
    "                    else (\n",
    "                        self.bound + self.adversary_radius\n",
    "                        if i == 0\n",
    "                        else -self.bound - self.adversary_radius\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    0.0\n",
    "                    if not i % 2\n",
    "                    else (\n",
    "                        self.bound + self.adversary_radius\n",
    "                        if i == 1\n",
    "                        else -self.bound - self.adversary_radius\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "            xform.set_rotation(torch.pi / 2 if not i % 2 else 0.0)\n",
    "            color = Color.BLACK.value\n",
    "            if isinstance(color, torch.Tensor) and len(color.shape) > 1:\n",
    "                color = color[env_index]\n",
    "            geom.set_color(*color)\n",
    "            geoms.append(geom)\n",
    "        return geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363c76aa-8cb4-4e33-9457-d29d6733dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100  # Environment steps before done\n",
    "\n",
    "n_chasers = 2\n",
    "n_evaders = 10\n",
    "n_obstacles = 2\n",
    "\n",
    "num_vmas_envs = (\n",
    "    frames_per_batch // max_steps\n",
    ")  # Number of vectorized environments. frames_per_batch collection will be divided among these environments\n",
    "base_env = VmasEnv(\n",
    "    scenario=PredationScenario(),\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=True,\n",
    "    max_steps=max_steps,\n",
    "    device=device,\n",
    "    seed=seed,\n",
    "    # Scenario specific\n",
    "    num_good_agents=n_evaders,\n",
    "    num_adversaries=n_chasers,\n",
    "    num_landmarks=n_obstacles,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "297b74c2-c030-480b-aa3b-0d6f97a19846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_map: {'adversary': ['adversary_0', 'adversary_1'], 'agent': ['agent_0', 'agent_1', 'agent_2', 'agent_3', 'agent_4', 'agent_5', 'agent_6', 'agent_7', 'agent_8', 'agent_9']}\n"
     ]
    }
   ],
   "source": [
    "print(f\"group_map: {base_env.group_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2e2a88-8d6f-49e6-b4c2-86b5cb64377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: Composite(\n",
      "    adversary: Composite(\n",
      "        action: BoundedContinuous(\n",
      "            shape=torch.Size([10, 2, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 2, 2]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 2, 2]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 2])),\n",
      "    agent: Composite(\n",
      "        action: BoundedContinuous(\n",
      "            shape=torch.Size([10, 10, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 10, 2]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 10, 2]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 10])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([10]))\n",
      "reward_spec: Composite(\n",
      "    adversary: Composite(\n",
      "        reward: UnboundedContinuous(\n",
      "            shape=torch.Size([10, 2, 1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 2])),\n",
      "    agent: Composite(\n",
      "        reward: UnboundedContinuous(\n",
      "            shape=torch.Size([10, 10, 1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 10, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 10, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 10])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([10]))\n",
      "done_spec: Composite(\n",
      "    done: Categorical(\n",
      "        shape=torch.Size([10, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: Categorical(\n",
      "        shape=torch.Size([10, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([10]))\n",
      "observation_spec: Composite(\n",
      "    adversary: Composite(\n",
      "        observation: UnboundedContinuous(\n",
      "            shape=torch.Size([10, 2, 50]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 2, 50]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 2, 50]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 2])),\n",
      "    agent: Composite(\n",
      "        observation: UnboundedContinuous(\n",
      "            shape=torch.Size([10, 10, 48]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 10, 48]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 10, 48]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([10, 10])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([10]))\n"
     ]
    }
   ],
   "source": [
    "print(\"action_spec:\", base_env.full_action_spec)\n",
    "print(\"reward_spec:\", base_env.full_reward_spec)\n",
    "print(\"done_spec:\", base_env.full_done_spec)\n",
    "print(\"observation_spec:\", base_env.observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22835b86-67a4-45bd-808b-3c0822fcf856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_keys: [('adversary', 'action'), ('agent', 'action')]\n",
      "reward_keys: [('adversary', 'reward'), ('agent', 'reward')]\n",
      "done_keys: ['done', 'terminated']\n"
     ]
    }
   ],
   "source": [
    "print(\"action_keys:\", base_env.action_keys)\n",
    "print(\"reward_keys:\", base_env.reward_keys)\n",
    "print(\"done_keys:\", base_env.done_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c6c92-9f55-4698-b134-bcdaa6b73679",
   "metadata": {},
   "source": [
    "## ENVIRONMENT TRANSFORMATION\n",
    "Transforms the environment so that it computes the sum of the reward and resets it when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d054c360-bcb0-4adb-8be5-ef4841802722",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    RewardSum(\n",
    "        in_keys=base_env.reward_keys,\n",
    "        reset_keys=[\"_reset\"] * len(base_env.group_map.keys()),\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc091e9-e4a8-43cc-8cb6-a8a114d85b37",
   "metadata": {},
   "source": [
    "## ENVIRONMENT SPECS TEST\n",
    "Makes sure that the environment fulfills the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53ebef1-cbcf-46c8-b440-194424962ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 15:24:16,102 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3cbb7-4514-4ab9-9b35-a60910e7beb7",
   "metadata": {},
   "source": [
    "# TEST ROLLOUT\n",
    "Tests the environment specification by performing a small dummy rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42434206-38d4-4420-8abb-8ea036bff532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of 5 steps: TensorDict(\n",
      "    fields={\n",
      "        adversary: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([10, 5, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 5, 2, 50]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 5, 2]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        agent: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([10, 5, 10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 5, 10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 5, 10, 48]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 5, 10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                adversary: TensorDict(\n",
      "                    fields={\n",
      "                        episode_reward: Tensor(shape=torch.Size([10, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        observation: Tensor(shape=torch.Size([10, 5, 2, 50]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([10, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([10, 5, 2]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                agent: TensorDict(\n",
      "                    fields={\n",
      "                        episode_reward: Tensor(shape=torch.Size([10, 5, 10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        observation: Tensor(shape=torch.Size([10, 5, 10, 48]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([10, 5, 10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([10, 5, 10]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([10, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10, 5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "Shape of the rollout TensorDict: torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "n_rollout_steps = 5\n",
    "rollout = env.rollout(n_rollout_steps)\n",
    "print(f\"rollout of {n_rollout_steps} steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f127c2-169c-45bc-a6d9-69cc05444599",
   "metadata": {},
   "source": [
    "# POLICY\n",
    "Creates the policy module for each agent group and sets up a policy for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ff4d15-3c18-4f9f-b4af-8cbf4a36eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_modules = {}\n",
    "for group, agents in env.group_map.items():\n",
    "    share_parameters_policy = True  # Can change this based on the group\n",
    "\n",
    "    policy_net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[group, \"observation\"].shape[\n",
    "            -1\n",
    "        ],  # n_obs_per_agent\n",
    "        n_agent_outputs=env.full_action_spec[group, \"action\"].shape[\n",
    "            -1\n",
    "        ],  # n_actions_per_agents\n",
    "        n_agents=len(agents),  # Number of agents in the group\n",
    "        centralised=False,  # the policies are decentralised (i.e., each agent will act from its local observation)\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    )\n",
    "\n",
    "    # Wrap the neural network in a :class:`~tensordict.nn.TensorDictModule`.\n",
    "    # This is simply a module that will read the ``in_keys`` from a tensordict, feed them to the\n",
    "    # neural networks, and write the\n",
    "    # outputs in-place at the ``out_keys``.\n",
    "\n",
    "    policy_module = TensorDictModule(\n",
    "        policy_net,\n",
    "        in_keys=[(group, \"observation\")],\n",
    "        out_keys=[(group, \"param\")],\n",
    "    )  # We just name the input and output that the network will read and write to the input tensordict\n",
    "    policy_modules[group] = policy_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "194c77f5-c50d-4027-a2de-bce3c558720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    policy = ProbabilisticActor(\n",
    "        module=policy_modules[group],\n",
    "        spec=env.full_action_spec[group, \"action\"],\n",
    "        in_keys=[(group, \"param\")],\n",
    "        out_keys=[(group, \"action\")],\n",
    "        distribution_class=TanhDelta,\n",
    "        distribution_kwargs={\n",
    "            \"low\": env.full_action_spec_unbatched[group, \"action\"].space.low,\n",
    "            \"high\": env.full_action_spec_unbatched[group, \"action\"].space.high,\n",
    "        },\n",
    "        return_log_prob=False,\n",
    "    )\n",
    "    policies[group] = policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e322ded-94f4-4f0a-bcab-572d0c930721",
   "metadata": {},
   "source": [
    "# EXPLORATION POLICIES\n",
    "Creates the exploration policies because DDMPG is deterministic and we still want to include some exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b83b1681-7a93-4861-93cb-7e31d2c9043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_policies = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    exploration_policy = TensorDictSequential(\n",
    "        policies[group],\n",
    "        AdditiveGaussianModule(\n",
    "            spec=policies[group].spec,\n",
    "            annealing_num_steps=total_frames\n",
    "            // 2,  # Number of frames after which sigma is sigma_end\n",
    "            action_key=(group, \"action\"),\n",
    "            sigma_init=0.9,  # Initial value of the sigma\n",
    "            sigma_end=0.1,  # Final value of the sigma\n",
    "        ),\n",
    "    )\n",
    "    exploration_policies[group] = exploration_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363c165-8c1f-4973-84ac-c7ad3cd352db",
   "metadata": {},
   "source": [
    "# CRITICS\n",
    "Creates the critics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b218217-1a9d-4b19-8a3c-738ec2d5bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "critics = {}\n",
    "for group, agents in env.group_map.items():\n",
    "    share_parameters_critic = True  # Can change for each group\n",
    "    MADDPG = True  # IDDPG if False, can change for each group\n",
    "\n",
    "    # This module applies the lambda function: reading the action and observation entries for the group\n",
    "    # and concatenating them in a new ``(group, \"obs_action\")`` entry\n",
    "    cat_module = TensorDictModule(\n",
    "        lambda obs, action: torch.cat([obs, action], dim=-1),\n",
    "        in_keys=[(group, \"observation\"), (group, \"action\")],\n",
    "        out_keys=[(group, \"obs_action\")],\n",
    "    )\n",
    "\n",
    "    critic_module = TensorDictModule(\n",
    "        module=MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[group, \"observation\"].shape[-1]\n",
    "            + env.full_action_spec[group, \"action\"].shape[-1],\n",
    "            n_agent_outputs=1,  # 1 value per agent\n",
    "            n_agents=len(agents),\n",
    "            centralised=MADDPG,\n",
    "            share_params=share_parameters_critic,\n",
    "            device=device,\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=torch.nn.Tanh,\n",
    "        ),\n",
    "        in_keys=[(group, \"obs_action\")],  # Read ``(group, \"obs_action\")``\n",
    "        out_keys=[\n",
    "            (group, \"state_action_value\")\n",
    "        ],  # Write ``(group, \"state_action_value\")``\n",
    "    )\n",
    "\n",
    "    critics[group] = TensorDictSequential(\n",
    "        cat_module, critic_module\n",
    "    )  # Run them in sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e8f3ede-27fa-48a6-af12-97d7f9c4c1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running value and policy for group 'adversary': TensorDict(\n",
      "    fields={\n",
      "        adversary: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([10, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                obs_action: Tensor(shape=torch.Size([10, 2, 52]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 2, 50]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                param: Tensor(shape=torch.Size([10, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                state_action_value: Tensor(shape=torch.Size([10, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 2]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        agent: TensorDict(\n",
      "            fields={\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 10, 48]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "Running value and policy for group 'agent': TensorDict(\n",
      "    fields={\n",
      "        adversary: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([10, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                obs_action: Tensor(shape=torch.Size([10, 2, 52]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 2, 50]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                param: Tensor(shape=torch.Size([10, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                state_action_value: Tensor(shape=torch.Size([10, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 2]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        agent: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([10, 10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                obs_action: Tensor(shape=torch.Size([10, 10, 50]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 10, 48]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                param: Tensor(shape=torch.Size([10, 10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                state_action_value: Tensor(shape=torch.Size([10, 10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "reset_td = env.reset()\n",
    "for group, _agents in env.group_map.items():\n",
    "    print(\n",
    "        f\"Running value and policy for group '{group}':\",\n",
    "        critics[group](policies[group](reset_td)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff98a3-2ae5-4ff7-91fc-b3322d4f4fb2",
   "metadata": {},
   "source": [
    "# COLLECTOR\n",
    "Creates the data collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a9cbad-fb79-4d5d-8c41-b44461f9097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put exploration policies from each group in a sequence\n",
    "agents_exploration_policy = TensorDictSequential(*exploration_policies.values())\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    agents_exploration_policy,\n",
    "    device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e03bd1-fb48-4d4e-a2c2-37839a8bcde2",
   "metadata": {},
   "source": [
    "# REPLAY BUFFERS\n",
    "Creates the replay buffers for the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df9d98d-ca35-406c-bddb-e55d2addeaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffers = {}\n",
    "scratch_dirs = []\n",
    "for group, _agents in env.group_map.items():\n",
    "    scratch_dir = tempfile.TemporaryDirectory().name\n",
    "    scratch_dirs.append(scratch_dir)\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        storage=LazyMemmapStorage(\n",
    "            memory_size,\n",
    "            scratch_dir=scratch_dir,\n",
    "        ),  # We will store up to memory_size multi-agent transitions\n",
    "        sampler=RandomSampler(),\n",
    "        batch_size=train_batch_size,  # We will sample batches of this size\n",
    "    )\n",
    "    if device.type != \"cpu\":\n",
    "        replay_buffer.append_transform(lambda x: x.to(device))\n",
    "    replay_buffers[group] = replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e947ba7-0d36-4646-bbd0-61e5c470dfc3",
   "metadata": {},
   "source": [
    "# OPTIMIZERS\n",
    "sets up the loss functions and optimizers for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ea6a8b4-fede-4e70-ab2f-476000b9ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    loss_module = DDPGLoss(\n",
    "        actor_network=policies[group],  # Use the non-explorative policies\n",
    "        value_network=critics[group],\n",
    "        delay_value=True,  # Whether to use a target network for the value\n",
    "        loss_function=\"l2\",\n",
    "    )\n",
    "    loss_module.set_keys(\n",
    "        state_action_value=(group, \"state_action_value\"),\n",
    "        reward=(group, \"reward\"),\n",
    "        done=(group, \"done\"),\n",
    "        terminated=(group, \"terminated\"),\n",
    "    )\n",
    "    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
    "\n",
    "    losses[group] = loss_module\n",
    "\n",
    "target_updaters = {\n",
    "    group: SoftUpdate(loss, tau=polyak_tau) for group, loss in losses.items()\n",
    "}\n",
    "\n",
    "optimisers = {\n",
    "    group: {\n",
    "        \"loss_actor\": torch.optim.Adam(\n",
    "            loss.actor_network_params.flatten_keys().values(), lr=lr\n",
    "        ),\n",
    "        \"loss_value\": torch.optim.Adam(\n",
    "            loss.value_network_params.flatten_keys().values(), lr=lr\n",
    "        ),\n",
    "    }\n",
    "    for group, loss in losses.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2ca9c-a29b-4535-bcc5-4158daa25669",
   "metadata": {},
   "source": [
    "# HELPER METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9ea3024-601e-4289-8fd0-748659c567c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch: TensorDictBase) -> TensorDictBase:\n",
    "    \"\"\"\n",
    "    If the `(group, \"terminated\")` and `(group, \"done\")` keys are not present, create them by expanding\n",
    "    `\"terminated\"` and `\"done\"`.\n",
    "    This is needed to present them with the same shape as the reward to the loss.\n",
    "    \"\"\"\n",
    "    for group in env.group_map.keys():\n",
    "        keys = list(batch.keys(True, True))\n",
    "        group_shape = batch.get_item_shape(group)\n",
    "        nested_done_key = (\"next\", group, \"done\")\n",
    "        nested_terminated_key = (\"next\", group, \"terminated\")\n",
    "        if nested_done_key not in keys:\n",
    "            batch.set(\n",
    "                nested_done_key,\n",
    "                batch.get((\"next\", \"done\")).unsqueeze(-1).expand((*group_shape, 1)),\n",
    "            )\n",
    "        if nested_terminated_key not in keys:\n",
    "            batch.set(\n",
    "                nested_terminated_key,\n",
    "                batch.get((\"next\", \"terminated\"))\n",
    "                .unsqueeze(-1)\n",
    "                .expand((*group_shape, 1)),\n",
    "            )\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30960af0-d33a-4c23-a643-581df40f0734",
   "metadata": {},
   "source": [
    "# TRAINING\n",
    "Trains the two groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144c102-54da-489a-bed4-b9d092295825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_reward_mean_adversary = 14.0, episode_reward_mean_agent = -1.399999976158142:  27%|▎| 27/100 [02:48<07:35,  6.24s"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(\n",
    "    total=n_iters,\n",
    "    desc=\", \".join(\n",
    "        [f\"episode_reward_mean_{group} = 0\" for group in env.group_map.keys()]\n",
    "    ),\n",
    ")\n",
    "episode_reward_mean_map = {group: [] for group in env.group_map.keys()}\n",
    "train_group_map = copy.deepcopy(env.group_map)\n",
    "\n",
    "# Training/collection iterations\n",
    "for iteration, batch in enumerate(collector):\n",
    "    current_frames = batch.numel()\n",
    "    batch = process_batch(batch)  # Util to expand done keys if needed\n",
    "    # Loop over groups\n",
    "    for group in train_group_map.keys():\n",
    "        group_batch = batch.exclude(\n",
    "            *[\n",
    "                key\n",
    "                for _group in env.group_map.keys()\n",
    "                if _group != group\n",
    "                for key in [_group, (\"next\", _group)]\n",
    "            ]\n",
    "        )  # Exclude data from other groups\n",
    "        group_batch = group_batch.reshape(\n",
    "            -1\n",
    "        )  # This just affects the leading dimensions in batch_size of the tensordict\n",
    "        replay_buffers[group].extend(group_batch)\n",
    "\n",
    "        for _ in range(n_optimiser_steps):\n",
    "            subdata = replay_buffers[group].sample()\n",
    "            loss_vals = losses[group](subdata)\n",
    "\n",
    "            for loss_name in [\"loss_actor\", \"loss_value\"]:\n",
    "                loss = loss_vals[loss_name]\n",
    "                optimiser = optimisers[group][loss_name]\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                # Optional\n",
    "                params = optimiser.param_groups[0][\"params\"]\n",
    "                torch.nn.utils.clip_grad_norm_(params, max_grad_norm)\n",
    "\n",
    "                optimiser.step()\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "            # Soft-update the target network\n",
    "            target_updaters[group].step()\n",
    "\n",
    "        # Exploration sigma anneal update\n",
    "        exploration_policies[group][-1].step(current_frames)\n",
    "\n",
    "    # Stop training a certain group when a condition is met (e.g., number of training iterations)\n",
    "    if iteration == iteration_when_stop_training_evaders:\n",
    "        del train_group_map[\"agent\"]\n",
    "\n",
    "    # Logging\n",
    "    for group in env.group_map.keys():\n",
    "        episode_reward_mean = (\n",
    "            batch.get((\"next\", group, \"episode_reward\"))[\n",
    "                batch.get((\"next\", group, \"done\"))\n",
    "            ]\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        episode_reward_mean_map[group].append(episode_reward_mean)\n",
    "\n",
    "    pbar.set_description(\n",
    "        \", \".join(\n",
    "            [\n",
    "                f\"episode_reward_mean_{group} = {episode_reward_mean_map[group][-1]}\"\n",
    "                for group in env.group_map.keys()\n",
    "            ]\n",
    "        ),\n",
    "        refresh=False,\n",
    "    )\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126e664-ea46-432e-bda5-174383890490",
   "metadata": {},
   "source": [
    "# VISUALISATION\n",
    "Visualizes the mean reward per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6f83b-557a-413d-92a6-c37f275ae9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "for i, group in enumerate(env.group_map.keys()):\n",
    "    axs[i].plot(episode_reward_mean_map[group], label=f\"Episode reward mean {group}\")\n",
    "    axs[i].set_ylabel(\"Reward\")\n",
    "    axs[i].axvline(\n",
    "        x=iteration_when_stop_training_evaders,\n",
    "        label=\"Agent (evader) stop training\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    axs[i].legend()\n",
    "axs[-1].set_xlabel(\"Training iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794ef34-856c-4a80-9e08-d74ce35a60ed",
   "metadata": {},
   "source": [
    "## Video\n",
    "Creates and saves a video replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e62e86-5743-4857-97a7-6c7127963964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace tmpdir with any desired path where the video should be saved\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    print(f\"tmp: {tmpdir}\")\n",
    "    tmpdir = \"/home/lilly/dev/marl-predator-prey/marl-predator-prey/videos\"\n",
    "    video_logger = CSVLogger(\"vmas_logs\", tmpdir, video_format=\"mp4\")\n",
    "    print(\"Creating rendering env\")\n",
    "    env_with_render = TransformedEnv(env.base_env, env.transform.clone())\n",
    "    env_with_render = env_with_render.append_transform(\n",
    "        PixelRenderTransform(\n",
    "            out_keys=[\"pixels\"],\n",
    "            # the np.ndarray has a negative stride and needs to be copied before being cast to a tensor\n",
    "            preproc=lambda x: x.copy(),\n",
    "            as_non_tensor=True,\n",
    "            # asking for array rather than on-screen rendering\n",
    "            mode=\"rgb_array\",\n",
    "        )\n",
    "    )\n",
    "    env_with_render = env_with_render.append_transform(\n",
    "        VideoRecorder(logger=video_logger, tag=\"vmas_rendered\")\n",
    "    )\n",
    "    with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
    "        print(\"Rendering rollout...\")\n",
    "        env_with_render.rollout(100, policy=agents_exploration_policy)\n",
    "    print(\"Saving the video...\")\n",
    "    env_with_render.transform.dump()\n",
    "    print(\"Saved! Saved directory tree:\")\n",
    "    video_logger.print_log_dir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18b25d-eb45-4990-95dc-b98e828feed5",
   "metadata": {},
   "source": [
    "# CLEANUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4600cf-19b1-4edc-a591-e612e2b37344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove scratch dir\n",
    "try:\n",
    "    import shutil\n",
    "\n",
    "    for scratch_dir in scratch_dirs:\n",
    "        # Use shutil.rmtree() to delete the directory and all its contents\n",
    "        shutil.rmtree(scratch_dir)\n",
    "        print(f\"Directory '{scratch_dir}' deleted successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory '{scratch_dir}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting directory: {e}\")\n",
    "# sphinx_gallery_end_ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d346936-fa20-490d-8010-2b3d54372a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
